<?xml version="1.0" encoding="utf-8"?><!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xml:lang="no" lang="no" epub:prefix="nordic: http://www.mtm.se/epub/">
<head xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:dcterms="http://purl.org/dc/terms/">
    <meta charset="UTF-8" />
    <title>Metode og dataanalyse</title>
    <meta name="dc:identifier" content="501325" />
    <meta name="viewport" content="width=device-width" />
    <meta name="nordic:guidelines" content="2015-1" />
    <meta name="nordic:supplier" content="Planman" />
    <meta name="nordic:supplieddate" content="2011-10-04" />
    <meta name="nordic:signum" content="" />
    <meta content="Gripsrud, Geir" name="dc:creator" />
    <meta content="Olsson, Ulf Henning" name="dc:creator" />
    <meta content="Silkoset, Ragnhild" name="dc:creator" />
    <meta content="2011-10-04" name="dc:date" />
    <meta content="NLB" name="dc:publisher" />
    <meta content="no" name="dc:language" />
    <meta content="urn:isbn:978-82-7634-864-4-1" name="dc:source" />
    <style type="text/css" xml:space="preserve">
                .initialism{
                    -epub-speak-as:spell-out;
                }
                .list-style-type-none{
                    list-style-type:none;
                }
                table[class ^= "table-rules-"],
                table[class *= " table-rules-"]{
                    border-width:thin;
                    border-style:hidden;
                }
                table[class ^= "table-rules-"]:not(.table-rules-none),
                table[class *= " table-rules-"]:not(.table-rules-none){
                    border-collapse:collapse;
                }
                table[class ^= "table-rules-"] td,
                table[class *= " table-rules-"] td{
                    border-width:thin;
                    border-style:none;
                }
                table[class ^= "table-rules-"] th,
                table[class *= " table-rules-"] th{
                    border-width:thin;
                    border-style:none;
                }
                table.table-rules-none td,
                table.table-rules-none th{
                    border-width:thin;
                    border-style:hidden;
                }
                table.table-rules-all td,
                table.table-rules-all th{
                    border-width:thin;
                    border-style:solid;
                }
                table.table-rules-cols td,
                table.table-rules-cols th{
                    border-left-width:thin;
                    border-right-width:thin;
                    border-left-style:solid;
                    border-right-style:solid;
                }
                table.table-rules-rows tr{
                    border-top-width:thin;
                    border-bottom-width:thin;
                    border-top-style:solid;
                    border-bottom-style:solid;
                }
                table.table-rules-groups colgroup{
                    border-left-width:thin;
                    border-right-width:thin;
                    border-left-style:solid;
                    border-right-style:solid;
                }
                table.table-rules-groups tfoot,
                table.table-rules-groups thead,
                table.table-rules-groups tbody{
                    border-top-width:thin;
                    border-bottom-width:thin;
                    border-top-style:solid;
                    border-bottom-style:solid;
                }
                table[class ^= "table-frame-"],
                table[class *= " table-frame-"]{
                    border:thin hidden;
                }
                table.table-frame-void{
                    border-style:hidden;
                }
                table.table-frame-above{
                    border-style:outset hidden hidden hidden;
                }
                table.table-frame-below{
                    border-style:hidden hidden outset hidden;
                }
                table.table-frame-lhs{
                    border-style:hidden hidden hidden outset;
                }
                table.table-frame-rhs{
                    border-style:hidden outset hidden hidden;
                }
                table.table-frame-hsides{
                    border-style:outset hidden;
                }
                table.table-frame-vsides{
                    border-style:hidden outset;
                }
                table.table-frame-box{
                    border-style:outset;
                }
                table.table-frame-border{
                    border-style:outset;
                }</style>
    <link rel="stylesheet" type="text/css" href="css/accessibility.css" />
    <link rel="prev" href="501325-15-chapter.xhtml" />
    <link rel="next" href="501325-17-chapter.xhtml" />
</head>
<body id="level2_10" epub:type="bodymatter chapter">
<div id="page-216" class="page-normal" epub:type="pagebreak" title="216"></div><h1 id="h2_10">Kapittel 10 Regresjonsanalyse</h1><section id="level3_58"><h2 id="h3_58">10.1 Innledning</h2><p>I dette avsnittet skal vi bruke f&#xf8;lgende symboler og begreper:</p><table id="table_28"><tbody><tr><td rowspan="1" colspan="1"><em>X</em></td><td rowspan="1" colspan="1">variabel X (uavhengig variabel)</td></tr><tr><td rowspan="1" colspan="1"><em>Y</em></td><td rowspan="1" colspan="1">variabel Y (avhengig variabel)</td></tr><tr><td rowspan="1" colspan="1"><em>&#x3b2;<sub>1</sub></em></td><td rowspan="1" colspan="1">regresjonsparameter</td></tr><tr><td rowspan="1" colspan="1"><em>&#x3b2;<sub>0</sub></em></td><td rowspan="1" colspan="1">konstantleddet</td></tr><tr><td rowspan="1" colspan="1"><span class="asciimath">`epsilon`</span></td><td rowspan="1" colspan="1">feilledd (forstyrrelsesledd)</td></tr><tr><td rowspan="1" colspan="1"><span class="asciimath">`ddotY`</span></td><td rowspan="1" colspan="1">predikert Y</td></tr><tr><td rowspan="1" colspan="1"><em>y</em></td><td rowspan="1" colspan="1">estimatet av <em>&#x3b2;</em></td></tr><tr><td rowspan="1" colspan="1"><em>X</em></td><td rowspan="1" colspan="1">andelen av forklart variabilitet</td></tr><tr><td rowspan="1" colspan="1"><em>e<sub>i</sub></em></td><td rowspan="1" colspan="1">estimatet av <span class="asciimath">`epsilon`</span> (ogs&#xe5; kalt residual)</td></tr><tr><td rowspan="1" colspan="1"><em>R</em><sup>2</sup></td><td rowspan="1" colspan="1">regresjonsligningens &#xab;forklaringskraft&#xbb;</td></tr><tr><td rowspan="1" colspan="1"><span class="asciimath">`barR^2`</span></td><td rowspan="1" colspan="1">justert regresjonsligningens &#xab;forklaringskraft&#xbb;</td></tr><tr><td rowspan="1" colspan="1"><em>TSS</em></td><td rowspan="1" colspan="1">total variabilitet</td></tr><tr><td rowspan="1" colspan="1"><em>RSS</em></td><td rowspan="1" colspan="1">variabilitet forklart av regresjonsmodellen</td></tr><tr><td rowspan="1" colspan="1"><em>ESS</em></td><td rowspan="1" colspan="1">summen av kvadrerte residualer</td></tr><tr><td rowspan="1" colspan="1"><em>F</em></td><td rowspan="1" colspan="1">F-testen</td></tr><tr><td rowspan="1" colspan="1"><em>H<sub>0</sub></em></td><td rowspan="1" colspan="1">nullhypotesen</td></tr><tr><td rowspan="1" colspan="1"><em>H<sub>1</sub></em></td><td rowspan="1" colspan="1">alternativhypotesen</td></tr><tr><td rowspan="1" colspan="1"><em>n</em></td><td rowspan="1" colspan="1">antall observasjoner</td></tr><tr><td rowspan="1" colspan="1">MKM</td><td rowspan="1" colspan="1">minste kvadraters metode (BLUE)</td></tr><tr><td rowspan="1" colspan="1"><em>t</em></td><td rowspan="1" colspan="1">testobservatoren til t-testen</td></tr><tr><td rowspan="1" colspan="1"><span class="asciimath">`SB`</span></td><td rowspan="1" colspan="1">standardfeilen til estimert <em>&#x3b2;</em></td></tr><tr><td rowspan="1" colspan="1">S<sub>e</sub></td><td rowspan="1" colspan="1">standardfeilen til resiudalen</td></tr><tr><td rowspan="1" colspan="1"><em>corr</em> (<em>X</em>,<em>Y</em>)</td><td rowspan="1" colspan="1">korrelasjonskoeffisienten mellom X og Y variablene</td></tr><tr><td rowspan="1" colspan="1"><em>k</em></td><td rowspan="1" colspan="1">antall uavhengige variabler</td></tr><tr><td rowspan="1" colspan="1"><em>i</em></td><td rowspan="1" colspan="1">i-te observasjon</td></tr></tbody></table><p>Regresjonsanalyse er en av mange statistiske metoder som benyttes for &#xe5; studere/evaluere sammenhengen mellom en eller flere s&#xe5;kalte uavhengige variabler <em>X</em><sub>1</sub>, <em>X</em><sub>2</sub>, <em>X</em><sub>3</sub>, ..., <em>X<sub>k</sub></em> og en avhengig kontinuerlig variabel <em>Y</em>. S&#xe6;rlig er man interesserti&#xe5;studere <span id="page-217" class="page-normal" epub:type="pagebreak" title="217"></span>hvordan endringerideuavhengigevariablene forklarer endringer i den avhengige variabelen. Anvendelsesomr&#xe5;dene er mange. Metoden kan for eksempel brukes til &#xe5; studere:</p><ol id="list_96" class="list-style-type-none"><li id="li_392">1 sammenhengen mellom pasienttilfredshet (<em>Y</em>) og de uavhengige variablene kommunikasjon mellom pasient og lege (<em>X</em><sub>1</sub>) og graden av service under sykehusoppholdet (<em>X</em><sub>2</sub>)</li><li id="li_393">2 sammenhengen mellom ettersp&#xf8;rselen etter en vare (<em>Y</em>) og de uavhengige variablene pris (<em>X</em><sub>1</sub>), reklameinnsats (<em>X</em><sub>2</sub>) og markedsandel (<em>X</em><sub>3</sub>)</li><li id="li_394">3 sammenhengen mellom blodtrykksnedsettende medikamenter (<em>X</em>) og endringen i blodtrykket (<em>Y</em>)</li><li id="li_395">4 sammenhengen mellom produktkvalitet (<em>X</em><sub>1</sub>), pris (<em>X</em><sub>2</sub>), kundetilfredshet (<em>X</em><sub>3</sub>) og lojalitet (<em>Y</em>)</li><li id="li_396">5 sammenhengen mellom avlingsmengde (<em>Y</em>), nedb&#xf8;rsmengde (<em>X</em><sub>1</sub>), temperatur (<em>X</em><sub>2</sub>) og nitrogeninnhold i jorda (<em>X</em><sub>3</sub>), etc.</li></ol><p>Flere av disse eksemplene er &#xe5;rsak&#x2013;virkningssammenhenger. Vi kan imidlertid aldri bevise noen &#xe5;rsakssammenheng med regresjonsanalyse, bare teste om mulige sammenhenger er <em>signifikant forskjellige fra null</em>. Vi tror for eksempel at reklameinnsats, pris, belegg (andelen solgte rom), konkurransen (antall konkurrenter) og service er <em>&#xe5;rsaker</em> til omsetningen p&#xe5; et hotell (se figur 10.1). Dette kan vi alts&#xe5; ikke <em>bevise.</em> Men vi kan benytte regresjonsanalyse til &#xe5; teste om det er en signifikant sammenheng mellom ettersp&#xf8;rselen og de fem forklarings-variablene.<a id="noteref_2" class="noteref" epub:type="noteref" href="501325-32-footnotes.xhtml#fn_217_1">1</a> Dersom vi finner signifikante sammenhenger, kan vi ogs&#xe5; benytte regresjonsanalyse til &#xe5; <em>predikere</em> omsetning for forskjellige verdier av reklame-innsats, pris, belegg, konkurranse og service.</p><p>Vi vil forvente at belegg, reklameinnsats og service vil ha en positiv effekt p&#xe5; omsetningen, mens pris og antall konkurrenter forventes &#xe5; ha en negativ effekt, se Figur 10.1.</p><figure id="imggroup_104" class="image"><img id="img_104" src="images/image104.jpg" alt="image" /><figcaption id="caption_102">Figur 10.1 Forskningsmodell over faktorer som p&#xe5;virker omsetning p&#xe5; hotell.</figcaption></figure><div id="page-218" class="page-normal" epub:type="pagebreak" title="218"></div><p>Det betyr at dersom belegget, reklameinnsatsen eller servicen blir &#xf8;kt eller bedret, s&#xe5; vil dette f&#xf8;re til at omsetningen ogs&#xe5; &#xf8;ker. En h&#xf8;yere pris og flere konkurrenter vil f&#xf8;re til det motsatte, alts&#xe5; en d&#xe5;rligere omsetning. Valget av uavhengige variabler skal v&#xe6;re teoretisk begrunnet. Med det mener vi at det skal v&#xe6;re mulig &#xe5; argumentere for og forsvare det valg man har gjort, ut fra for eksempel &#xf8;konomisk teori (eller sunn fornuft). P&#xe5; denne m&#xe5;ten vil regresjonsanalysen ogs&#xe5; fungere som en <em>teoritest</em>.</p><p>Det er mest vanlig &#xe5; anta at sammenhengene er line&#xe6;re. Det betyr at omsetningen antas &#xe5; v&#xe6;re en <em>line&#xe6;r funksjon</em> av de uavhengige variablene. Videre vil man v&#xe6;re &#xe5;pen for at det er flere faktorer enn de fem som vil kunne p&#xe5;virke omsetningen. Dette betyr at belegg, pris, reklame, konkurranse og service ikke <em>forklarer</em> alt. For &#xe5; ta hensyn til dette opererer vi med et <em>feilledd</em> symbolisert med den greske bokstaven <em>e</em>, som skal representere det <em>uforklarte</em>. Den fullstendige regresjonsligningen blir dermed seende slik ut:</p><p><em>Omsetning</em> = <em>&#x3b2;</em><sub>0</sub> + <em>&#x3b2;</em><sub>1</sub><em>Belegg</em> + <em>&#x3b2;</em><sub>2</sub><em>Pris</em> + <em>&#x3b2;</em><sub>3</sub><em>Reklame</em> + <em>&#x3b2;</em><sub>4</sub><em>Konkurranse</em> + <em>&#x3b2;</em><sub>5</sub><em>Service</em> +<em>&#x3b5;</em></p><p><em>&#x3b2;</em><sub>0</sub> kalles konstantledd, <em>&#x3b2;</em><sub>i</sub> (<em>i</em> = 1, 2, 3, 4, 5) kalles regresjonskoeffisienter eller regresjonsparametre, og <em>&#x3b5;</em> kaller vi <em>feilleddet</em> (forstyrrelsesleddet). H&#xf8;yresiden i ligningen best&#xe5;r av to deler, den <em>forklarte</em> delen representert ved:</p><p><em>&#x3b2;</em><sub>0</sub> + <em>&#x3b2;</em><sub>1</sub><em>Belegg</em> + <em>&#x3b2;</em><sub>2</sub><em>Pris</em> + <em>&#x3b2;</em><sub>3</sub><em>Reklame</em> + <em>&#x3b2;</em><sub>4</sub><em>Konkurranse</em> + <em>&#x3b2;</em><sub>5</sub><em>Service</em></p><p>og den <em>uforklarte</em> delen representert ved feilleddet <em>e</em>. Legg merke til at selv om vi forventer en negativ sammenheng mellom pris og omsetning, har vi plusstegn i regresjonsligningen. Dette er fordi den negative sammenhengen gjelder for koeffisienten.</p><p>Det er &#xf8;nskelig at den uforklarte delen skal v&#xe6;re av underordnet betydning sammenlignet med den forklarte delen. Vi &#xf8;nsker med andre ord en regresjons-ligning med s&#xe5; stor <em>forklaringskraft</em> som mulig. Dette vil vi komme tilbake til.</p><p>La oss, f&#xf8;r vi g&#xe5;r over p&#xe5; en mer teknisk beskrivelse av regresjonsanalysen, ogs&#xe5; nevne hvordan vi kan tolke konstantleddet og regresjonskoeffisientene. Konstantleddet <em>&#x3b2;</em><sub>0</sub> angir omsetningen n&#xe5;r <em>belegg, pris, reklame, konkurranse</em> og <em>service</em> alle antar verdien null. Ofte er det vanskelig &#xe5; gi konstantleddet noe mer enn en matematisk fortolkning. Det er imidlertid feil &#xe5; ekskludere det fra regre-sjonsligningen. Senere i kapitlet vil vi gi et eksempel p&#xe5; n&#xe5;r konstantleddet kan v&#xe6;re nyttig.</p><p>Regresjonskoeffisientene <em>&#x3b2;<sub>i</sub></em> (<em>i</em> = 1, 2, 3, 4, 5) angir den &#xab;isolerte&#xbb; effekten som hver forklaringsvariabel har p&#xe5; omsetningen. For eksempel vil <em>&#x3b2;</em><sub>3</sub> fortelle hvor stor effekt en enhetsendring av <em>reklame</em> vil ha p&#xe5; omsetningen. Her m&#xe5; vi <span id="page-219" class="page-normal" epub:type="pagebreak" title="219"></span>imidlertid forutsette at de andre variablene, <em>belegg, pris, konkurranse</em> og <em>service</em> holdes konstante. Dersom <em>&#x3b2;</em><sub>3</sub> har blitt beregnet til &#xe5; v&#xe6;re for eksempel 2,09 og vi antar at <em>reklameinnsatsen</em> og salget m&#xe5;les i enheter p&#xe5; millioner, vil det &#xe5; &#xf8;ke reklameinnsatsen med &#xe9;n million kroner medf&#xf8;re en salgs&#xf8;kning p&#xe5; i overkant av to millioner kroner, mer presist 2 090 000, forutsatt at de andre variablene ikke endres.</p><p>N&#xe5; er det imidlertid slik at det er den <em>forventede</em> omsetnings&#xf8;kningen vi m&#xe5;ler. Det vil si at en &#xf8;kning i reklameinnsatsen p&#xe5; &#xe9;n million kroner vil f&#xf8;re til en forventet omsetnings&#xf8;kning p&#xe5; to millioner kroner. Dette blir presisert nedenfor.</p><p>Generelt er regresjonsmetoden den samme om vi arbeider med problemstillinger fra s&#xe5; vidt forskjellige fagomr&#xe5;der som finans, medisin, psykologi, jordbruk, idrett etc. Det er derfor naturlig at noen generelle statistiske <em>forutsetninger</em> m&#xe5; v&#xe6;re oppfylt for at metoden skal fungere slik vi &#xf8;nsker. Disse <em>forutsetningene</em> (<em>klassiske antakelsene</em>) finner du i vedlegget bakerst i kapitlet.</p></section><section id="level3_59"><h2 id="h3_59">10.2 Enkel regresjonsanalyse</h2><p>La oss tenke oss at vi observerer resultatene av en stokastisk (tilfeldig) prosess <em>F</em> hvor det inng&#xe5;r en avhengig variabel <em>Y</em> og flere uavhengige variabler <em>X</em><sub>1</sub>, <em>X</em><sub>2</sub>, <em>X</em><sub>3</sub>, ..., <em>X<sub>k</sub></em>. Da kan vi for eksempel skrive <em>Y</em> = <em>F</em>(<em>X</em><sub>1</sub>, <em>X</em><sub>2</sub>, <em>X</em><sub>3</sub>, ... , <em>X<sub>k</sub></em>). Dette betyr at <em>Y</em> er en funksjon av mange variabler (flere enn vi aner) <em>X</em><sub>1</sub>, <em>X</em><sub>2</sub>, <em>X</em><sub>3</sub>, ... , <em>X<sub>k</sub></em>. Vi kaller ligningen &#xab;den sanne modell&#xbb;. Den er vanligvis ukjent, men vi kan ha en formening om hvordan den ser ut. Dersom vi mener at modellen er <em>line&#xe6;r,</em> og at <em>X</em><sub>2</sub>, <em>X</em><sub>3</sub>, <em>X</em><sub>4</sub>, ... er av underordnet betydning i forhold til <em>X</em><sub>1</sub> (dette er ikke n&#xf8;dvendigvis riktig), er det naturlig at v&#xe5;r teoretiske modell ser slik ut:</p><figure id="imggroup_105" class="image"><img id="img_105" src="images/image105.jpg" alt="image" /><figcaption id="caption_103">Figur 10.2 Regresjonslinje</figcaption></figure><div id="page-220" class="page-normal" epub:type="pagebreak" title="220"></div><p><em>Y<sub>i</sub></em> =<em>&#x3b2;</em><sub>0</sub> +<em>&#x3b2;</em><sub>1</sub> <em>X</em><sub>1<em>i</em></sub> +<em>&#x3b5;<sub>i</sub></em> , for observasjon nr. <em>i</em>.</p><p>Den forklarte delen <em>&#x3b2;</em><sub>0</sub> + <em>&#x3b2;X</em><sub>1</sub> kan framstilles grafisk som en rett linje i et koordinatsystem hvor <em>&#x3b2;</em><sub>0</sub> og <em>&#x3b2;</em><sub>1</sub> er ukjente parametre som representerer hhv. kons-tantleddet og stigningstallet (se figur 10.2). Dette er st&#xf8;rrelser som vi &#xf8;nsker &#xe5; bestemme. N&#xe5;r vi &#xab;velger&#xbb; &#xe5; se bort ifra <em>X</em><sub>2,</sub> <em>X</em><sub>3</sub>, <em>X</em><sub>4</sub>, ... , vil en del av variasjonen i <em>Y</em> ikke finne sin forklaring i den forenklede modellen <em>&#x3b2;</em><sub>0</sub> + <em>&#x3b2;X</em><sub>1</sub>. Denne uforklarte variasjonen vil da komme fra de <em>uteglemte variablene</em>, men ogs&#xe5; fra eventuelle <em>m&#xe5;lefeil, feilaktige funksjonsformer</em> eller <em>rene tilfeldigheter</em>. Ved &#xe5; &#xab;innr&#xf8;mme&#xbb; at det finnes uforklart variasjon, inkluderes <em>&#x3b5;</em><sub>i</sub> ligningen. Dermed representerer <em>&#x3b5;</em> p&#xe5; mange m&#xe5;ter det som er uteglemt i v&#xe5;r modell.</p><p>Rent formelt sier vi at <em>e</em> symboliserer et tilfeldig (stokastisk) forstyrrelsesledd (feilledd).</p></section><section id="level3_60"><h2 id="h3_60">10.3 Estimering av regresjonsparametrene</h2><p>I forklaringen av line&#xe6;r regresjon tar vi utgangspunkt i hotelldataene hotell.jmp for &#xe5; bruke som gjennomgangseksempel. Vi antar f&#xf8;lgende forskningsmodell:</p><figure id="imggroup_106" class="image"><img id="img_106" src="images/image106.jpg" alt="image" /></figure><p>Hypotesen skriver vi ned p&#xe5; f&#xf8;lgende m&#xe5;te:</p><p>Hypotese 1: Beleggsprosent har en positiv p&#xe5;virkning p&#xe5; omsetning.</p><p>Vi antar med andre ord en positiv line&#xe6;r sammenheng; jo flere solgte rom (beleggsprosent), jo h&#xf8;yere omsetning p&#xe5; hotellet (m&#xe5;lt i millioner kroner). <em>Noter at vi har ekskludert case 99 og 122 p&#xe5; grunn av ekstremverdier.</em> Vi kj&#xf8;rer reg-resjonsligningen i JMP</p><p><em>Analyze</em> &gt; <em>Fit Model</em></p><p>Vi legger inn avhengig variabel under <em>Y</em>, i v&#xe5;rt tilfelle omsetning, og uavhengige variabler under <em>Add</em>, se figur 10.3.</p><div id="page-221" class="page-normal" epub:type="pagebreak" title="221"></div><figure id="imggroup_107" class="image"><img id="img_107" src="images/image107.jpg" alt="image" /><figcaption id="caption_104">Figur 10.3 Dialogboks for line&#xe6;r regresjon i JMP.</figcaption></figure><p>Utskriften blir som f&#xf8;lger:</p><figure id="imggroup_108" class="image"><img id="img_108" src="images/image108.jpg" alt="image" /><figcaption id="caption_105">Tabell 10.1 Utskrift fra enkel regresjonsanalyse i JMP.</figcaption></figure><div id="page-222" class="page-normal" epub:type="pagebreak" title="222"></div><p>Vi skal bruke utskriften fra tabell 10.1 til de ulike forklaringene i dette kapitlet. I tabell 10.1 ser vi at det estimerte konstantleddet, <em>&#x3b2;<sub>0</sub>,</em> er -7,726, mens den estimerte regresjonskoeffisienten for beleggsprosent, <em>&#x3b2;</em><sub>belegg</sub>, er 0,490. Dette gir f&#xf8;lgende regresjonsligning:</p><p><span class="asciimath">`Y_i = stackrel^^beta_0 + stackrel^^betaX_i iff "Omsetning" = -7,726 + 0,490`</span> <em>beleggsprosent</em></p><p>Dersom hotellet har 0 i beleggsprosent, har de -7 726 000 kroner i omsetning, eller sagt med andre ord et underskudd p&#xe5; 7 726 000. Dersom beleggsprosenten &#xf8;ker med &#xe9;n enhet, i dette tilfellet &#xe9;n prosent, forventes omsetningen &#xe5; &#xf8;ke med 0,490 enheter (dvs. 490 000 kroner fordi det er m&#xe5;lt i millioner). Ved en beleggsprosent p&#xe5; 40 kan man predikere at hotellet har f&#xf8;lgende omsetning: Vi setter inn verdien 40 for X og finner f&#xf8;lgende:</p><p>-7,726 + (0,490 &#x2022; 40) = 11,874, det vil si 11 874 000 kroner.</p><p>I hotelleksemplet v&#xe5;rt har vi <em>estimert</em> hvor stor omsetning man kan forvente p&#xe5; hotellet ved en gitt beleggsprosent. Denne estimeringen visualiseres gjennom den linjen som er trukket i tabell 10.1. Denne regresjonslinjen har et konstantledd <em>&#x3b2;<sub>0</sub></em> og et stigningstall <em>&#x3b2;</em><sub>i</sub>. Estimering handler nettopp om &#xe5; bestemme de ukjente parametrene <em>&#x3b2;<sub>0</sub> &#x3b2;</em> og <em>&#x3b5;<sub>i</sub>.</em> N&#xe5; er det slik at vi sjelden eller aldri har tilgang til hele populasjonen, men m&#xe5; n&#xf8;ye oss med et utvalg (stikkpr&#xf8;ve). Derfor er det umulig &#xe5; beregne de virkelige verdiene p&#xe5; <em>&#x3b2;<sub>0</sub> &#x3b2;</em> og <em>&#x3b5;<sub>i</sub>.</em> I stedet m&#xe5; de <em>estimeres</em> med data fra stikkpr&#xf8;ven, og som symboler for estimater benytter vi <span class="asciimath">`stackrel^^beta_0,stackrel^^beta`</span> <em>og e<sub>i</sub></em>.</p><p>Vi har benyttet den mest brukte estimeringsmetoden, som er minste kvadraters metode (MKM). Ideen bak denne metoden er &#xe5; &#xab;finne fram&#xbb; til estimater som er slik at summen av de kvadrerte residualene (blir forklart n&#xe6;rmere nedenfor) blir s&#xe5; liten som mulig. Det kan vises at dette er oppfylt ved estimater gitt ved formlene:</p><p><span class="asciimath">`stackrel^^beta = (sum xy)/(sum x^2) "og"\ stackrel(^^)beta_0 = barY - stackrel(^^)beta barX`</span></p><p><em>x</em> og <em>y</em> definert p&#xe5; denne m&#xe5;ten kalles ofte avviksscorer (&#xab;gjennomsnittssent-rering&#xbb;). I formelen er <span class="asciimath">`x`</span> <span class="asciimath">`=`</span> <span class="asciimath">`X -barX`</span><em> og</em> <span class="asciimath">`y`</span> <span class="asciimath">`=`</span> <span class="asciimath">`Y - barY`</span>, mens <span class="asciimath">`barX`</span> og <span class="asciimath">`barY`</span> er de respektive gjennomsnittsverdiene. Visuelt betyr dette &#xe5; bestemme helningsgraden p&#xe5; regresjonslinjen slik at <em>summen av de kvadrerte residualene</em> (estimerte feilledd) blir minimale.</p><p>Estimatet for feilleddet <em>e</em><sub>i</sub> kan beregnes ved differansen <span class="asciimath">`e_i = Y_i - (stackrel^^beta_0 + stackrel^^betaX_i)`</span><span class="asciimath">`X_i)`</span>. Denne differansen kalles residualet. Leddet <em>&#x3b2;</em><sub>0</sub> <em>+ &#x3b2;<sub>i</sub></em> symboliseres vanligvis med</p><p><span class="asciimath">`stackrel^^Y_i`</span>, slik at <span class="asciimath">`stackrel^^Y_i = stackrel^^beta_0 + stackrel^^betaX_i`</span>__. Med andre ord f&#xe5;r vi at estimatet for feilleddet <em>&#x3b5;</em> er lik</p><p>Y minus estimatet for <span class="asciimath">`stackrel^^Y`</span>, det vil si <span class="asciimath">`e_i =Y_i-stackrel^^Y_i`</span></p><div id="page-223" class="page-normal" epub:type="pagebreak" title="223"></div><p>EKSEMPEL 10.1</p><p>La oss anta at sammenhengen mellom kilopris p&#xe5; epler (<em>X</em>) og salg (<em>Y</em>) er line&#xe6;r. Dette gir oss f&#xf8;lgende ligning: <span class="asciimath">`Salg_i = beta_0 - betaPris_i + epsilon_i`</span>. Vi tenker oss at dataene i tabellen er samlet inn over en tidagersperiode, og at prisen varierte hver dag, se tabell 10.2. Salget er m&#xe5;lt i antall kilo solgte epler, og prisen er per kilo epler.</p><table id="table_29"><tbody><tr><td rowspan="1" colspan="1">Salg</td><td rowspan="1" colspan="1">34</td><td rowspan="1" colspan="1">30</td><td rowspan="1" colspan="1">36</td><td rowspan="1" colspan="1">35</td><td rowspan="1" colspan="1">22</td><td rowspan="1" colspan="1">21</td><td rowspan="1" colspan="1">25</td><td rowspan="1" colspan="1">30</td><td rowspan="1" colspan="1">20</td><td rowspan="1" colspan="1">16</td></tr><tr><td rowspan="1" colspan="1">Pris</td><td rowspan="1" colspan="1">12</td><td rowspan="1" colspan="1">14</td><td rowspan="1" colspan="1">10</td><td rowspan="1" colspan="1">10</td><td rowspan="1" colspan="1">15</td><td rowspan="1" colspan="1">15</td><td rowspan="1" colspan="1">20</td><td rowspan="1" colspan="1">15</td><td rowspan="1" colspan="1">20</td><td rowspan="1" colspan="1">20</td></tr></tbody></table><p>Tabell 10.2 Solgte enheter og pris.</p><p>Den estimerte sammenhengen med minste kvadraters metode blir:</p><p><span class="asciimath">`stackrel(^^)Salg_i = 503 - 1,55Pris_i`</span></p><p>Uttrykket 50,3 &#x2013; 1,55<em>Pris</em> tolker vi som det <em>forventede</em> salg for en gitt pris. Om prisen p&#xe5; epler er 12 kroner kiloen i dag, og vi &#xf8;ker den to kroner, til 14 kroner kiloen, vil salget reduseres med 3,1 kilo (-1,55 x 2 = -3,1).</p><p><em>Stigningstallet,</em> som er lik -1,55, blir den forventede salgs&#xf8;kningen n&#xe5;r prisen &#xf8;ker med enhet. Siden denne verdien er negativ, kan vi forvente at salget avtar n&#xe5;r prisen &#xf8;ker, se figur 10.4. Dette virker rimelig. Konstantleddet forteller at <em>dersom prisen er null, vil salget v&#xe6;re 50,3</em>. Dette virker noks&#xe5; meningsl&#xf8;st. Som vi nevnte innledningsvis, er det sjelden interessant &#xe5; gi konstantleddet noe mer enn en matematisk fortolkning. Neste eksempel viser imidlertid at det noen ganger kan gi viktig informasjon.</p><figure id="imggroup_109" class="image"><img id="img_109" src="images/image109.jpg" alt="image" /><figcaption id="caption_106">Figur 10.4 Sammenhengen mellom salg og pris.</figcaption></figure><div id="page-224" class="page-normal" epub:type="pagebreak" title="224"></div><p>EKSEMPEL 10.2</p><p>Her skal vi vise et eksempel hvor konstantleddet har betydning. La oss anta at vi har estimert f&#xf8;lgende regresjonsmodell for sammenhengen mellom l&#xf8;nnsforskjellen mellom kvinner og menn (<em>Y</em>) og forklaringsvariabelen antall &#xe5;r ansatt i en kunnskapsbedrift <em>(X):</em></p><p><span class="asciimath">`stackrel(^^)Y_i = 10 + 5,5X_i`</span></p><p>Dersom <em>Y</em> m&#xe5;les som differansen L&#xf8;nn_kvinne &#x2013; L&#xf8;nn_mann i enheter p&#xe5; 1 000 kr, forteller konstantleddet at kvinner f&#xe5;r en begynnerl&#xf8;nn som i gjennomsnitt ligger 10 000 kr <span class="asciimath">`(stackrel^^beta_0 = 10)`</span> h&#xf8;yere enn for menn. Videre viser stigningstallet at denne forskjellen &#xf8;ker i gjennomsnitt med 5 500 kr per &#xe5;r <span class="asciimath">`(stackrel^^beta = 5,5)`</span>.</p></section><section id="level3_61"><h2 id="h3_61">10.4 t-testen</h2><p>Tenk deg at et hotell bestemmer seg for &#xe5; &#xf8;ke servicen p&#xe5; hotellet for &#xe5; f&#xe5; opp omsetningen. Dette gj&#xf8;r de gjennom et stort prosjekt som omfatter kursing av alle ansatte. S&#xe5; viser det seg at tiltaket ikke f&#xf8;rer til noen &#xf8;kt omsetning i det hele tatt. Det som kan ha skjedd er at regresjonskoeffisienten <span class="asciimath">`beta_(service)`</span> ikke har hatt noen signifikant p&#xe5;virkning p&#xe5; <em>Y</em>, omsetningen. Slike feil strategiske valg kan f&#xe5; store &#xf8;konomiske konsekvenser for en bedrift. For &#xe5; sikre seg mot slike Type I-feil (se kapittel 9), tester man signifikansen til hver av regresjonskoef-fisientene. Vi tester derved om det er noen sammenheng mellom <em>X</em> og <em>Y.</em> Dette gj&#xf8;r vi ved &#xe5; teste om stigningstallet <em>&#x3b2;</em> er <em>signifikant forskjellig fra null.</em> Dersom vi kan konkludere med at <em>&#x3b2;</em> er null, vil dette opplagt v&#xe6;re en sterk indikasjon p&#xe5; at de to variablene er fullstendig uavhengige av hverandre.</p><p>I regresjonsanalyse er denne testen atskillig mer benyttet enn <em>F</em>-testen som vi beskriver litt senere i kapitlet. S&#xe6;rlig i multippel regresjonsanalyse, hvor man er interessert i &#xe5; teste de individuelle stigningskoeffisientene (<em>&#x3b2;<sub>k</sub></em>), er den viktig. Testen gjennomf&#xf8;res imidlertid likt i enkel og multippel regresjon. I v&#xe5;rt tilfelle vil vi teste f&#xf8;lgende nullhypotese om stigningstallet: <em>H<sub>0</sub>: &#x3b2; =</em> 0. Nullhypotesen p&#xe5;st&#xe5;r derved f&#xf8;lgende:</p><p><span class="asciimath">`Y_i = beta_0 + 0 * X_i + epsilon_i iff Y_i = beta_0 + epsilon_i`</span></p><p><em>Y</em><sub>i</sub> er med andre ord lik en konstant pluss et tilfeldig feilledd. Dette er det samme som &#xe5; si at effekten fra en endring i <em>X ikke</em> har noen betydning for <em>Y</em>-verdien. Dette tolker vi som at det ikke er noen sammenheng mellom <em>X</em> og <em>Y</em> . Som alternativ hypotese er det tre muligheter, ett tosidig alternativ og to ensidige alternativ.</p><p>La oss ta det <em>tosidige</em> f&#xf8;rst. Det kan v&#xe6;re flere grunner til &#xe5; velge en tosidig <span id="page-225" class="page-normal" epub:type="pagebreak" title="225"></span>test, for eksempel den at du p&#xe5; forh&#xe5;nd ikke har noen klar oppfatning av om <em>X</em> har en positiv eller negativ effekt p&#xe5; <em>Y</em>. Du er alts&#xe5; i tvil om <em>b</em> er positiv eller negativ. Da vil det v&#xe6;re naturlig &#xe5; p&#xe5;st&#xe5; i den alternative hypotesen at <em>&#x3b2;</em> er ulik (&#x2260;) null. Har du p&#xe5; forh&#xe5;nd har en klar formening om fortegnet p&#xe5; stigningskoeffi-sienten, er det naturlig &#xe5; velge en <em>ensidig</em> test. Da vil den alternative hypotesen enten p&#xe5;st&#xe5; at <em>&#x3b2;</em> er positiv, eller at <em>&#x3b2;</em> er negativ. Gjennomf&#xf8;ringen av selve testen og beregningen av testobservatoren er den samme for de to situasjonene. For &#xe5; teste nullhypotesen vil vi benytte t-testen, da det kan vises at testobservatoren er t-fordelt med <em>n -</em> 2 frihetsgrader. Testobservatoren (<em>t</em>) finner vived &#xe5; dele estimert regresjonskoeffisient <span class="asciimath">`(stackrel^^beta)`</span> p&#xe5; standardfeilen til den estimerte regresjonskoeffisi-enten <span class="asciimath">`(s_stackrel^^beta)`</span> (utledningen av formelen finnes i vedlegget til kapitlet).</p><p><span class="asciimath">`t = stackrel(^^)beta/(s_(stackrel(^^)beta))`</span></p><p>La oss ta for oss de tre testsituasjonene:</p><section id="level4_74"><h3 id="h4_74">1 Tosidig test positiv eller negativ (=)</h3><p><em>H</em><sub>0</sub>: <em>&#x3b2;</em> = 0 og <em>H<sub>1</sub></em>: <em>b</em> &#x2260; 0. <em>H</em><sub>0</sub> forkastes dersom |<em>t/ &gt; t</em><sub>&#x3b1;</sub>. Vi p&#xe5;st&#xe5;r da at <em>&#x3b2;</em> er <em>ulik</em> null. Legg merke til at her er fortegnet p&#xe5; <em>b</em> uten betydning da vi tester begge sidene. Antall frihetsgrader er <em>n</em> &#x2013; 2. Tabellen for <em>t</em>-fordelingen i boka (vedlegg 3B) viser arealet til h&#xf8;yre for <em>t</em>. For &#xe5; f&#xe5; tosidig test m&#xe5; vi derfor dele signifikansniv&#xe5;et p&#xe5; 2. Dette betyr at et 5 % signifikansniv&#xe5; best&#xe5;r av 2,5 % av arealet til venstre og 2,5 % av arealet til h&#xf8;yre for <em>t</em>.</p></section><section id="level4_75"><h3 id="h4_75">2 Ensidig test positiv (&gt;)</h3><p><em>H</em><sub>0</sub>: <em>&#x3b2;</em> = 0 og <em>H<sub>1</sub></em> : <em>&#x3b2; &gt;</em> 0. <em>H</em><sub>0</sub> forkastes dersom <em>t &gt; t</em><sub>&#x3b1;</sub>. Vi p&#xe5;st&#xe5;r da at <em>&#x3b2;</em> er <em>st&#xf8;rre enn</em> null, eller at det er en positiv sammenheng mellom <em>X</em> og <em>Y</em>. Antall frihetsgrader er <em>n -</em> 2.</p></section><section id="level4_76"><h3 id="h4_76">3 Ensidig test negativ ( &lt; )</h3><p><em>H</em><sub>0</sub>: <em>&#x3b2;</em> = 0 og <em>H</em><sub>1</sub>: <em>&#x3b2;  &lt; </em> 0. <em>H</em><sub>0</sub> forkastes dersom <em>t  &lt;  -t</em><sub>&#x3b1;</sub>. Vi p&#xe5;st&#xe5;r da at <em>&#x3b2;</em> er <em>mindre enn</em> null, eller at det er en negativ sammenheng mellom <em>X</em> og <em>Y</em>. Legg merke til minustegnet (-) foran <em>t</em><sub>&#x3b1;</sub>. Det betyr at <em>H</em><sub>0</sub> kun kan forkastes n&#xe5;r <em>t</em> er negativ i denne situasjonen. Antall frihetsgrader er <em>n -</em> 2.</p><p><em>t<sub>&#x3b1;</sub></em> finner du i vedlegg 3B (<em>t</em>-fordelingen). Merk at <em>t<sub>&#x3b1;</sub></em> ikke er den samme for tosidige og ensidige tester.</p><div id="page-226" class="page-normal" epub:type="pagebreak" title="226"></div><p>EKSEMPEL 10.3</p><p>La oss f&#xf8;lge opp eksemplet v&#xe5;rt med epler, og teste om prisen har noen effekt p&#xe5; salget av epler. Vi antar at jo h&#xf8;yere pris vi tar p&#xe5; epler, jo lavere salg av epler vil vi ha. Ordlyden i hypotesen blir derved f&#xf8;lgende:</p><p>Hypotese<sub>1</sub>: Prisen har en negativ effekt p&#xe5; salget av epler.</p><p>Formelt tester vi f&#xf8;lgende hypotese:</p><p><em>H</em><sub>0</sub>:<em>&#x3b2;</em> =0 og <em>H</em><sub>1</sub>: <em>&#x3b2;  &lt; </em> 0</p><p>Vi tester med et 5 % signifikansniv&#xe5;. P&#xe5; basis av utvalget v&#xe5;rt (stikkpr&#xf8;ven) f&#xe5;r vi f&#xf8;lgende verdier ved hjelp av MKM-beregning:</p><p><span class="asciimath">`stackrel^^beta_0 = 50,28\ "og"\ stackrel^^beta_1 = -1,55`</span></p><p>Vi kan regne ut <em>t</em>-observatoren manuelt ved &#xe5; bruke formelen:</p><p><span class="asciimath">`t = stackrel(^^)beta/(s_(stackrel(^^)beta)) = (-1,55)/(0,338) = -4,58`</span></p><p>Den samme verdien blir imidlertid ogs&#xe5; oppgitt direkte i JMP-utskriften. Det er ogs&#xe5; mulig &#xe5; sjekke <em>t</em>-verdien opp mot den kritiske <em>t<sub>&#x3b1;</sub></em> p&#xe5; 5 %-niv&#xe5;et (vedlegg 3B) manuelt. Frihetsgradstallet er 8 <em>(n</em> - 2<em> =</em> 10 - 2 = 8). Kritisk <em>t</em> blir p&#xe5; -1,860 (husk at vi tester en negativ t n&#xe5;r vi antar en negativ effekt i hypotesen). Fordi <em>t  &lt;  -t<sub>&#x3b1;</sub>,</em> det vil si -1,860  &lt;  - (-4,58), kan vi forkaste nullhypotesen.</p><p><em>Konklusjonen</em> er derved at prisen p&#xe5; epler har en negativ effekt p&#xe5; salget av epler, og den endelige regresjonsligningen er som f&#xf8;lger:</p><p><em>Salg =</em> 50,28 - 1,55 <em>pris</em></p><p>EKSEMPEL 10.4</p><p>I eksemplet over fant vi at for hver gang prisen p&#xe5; epler &#xf8;ker med &#xe9;n krone, selger man 1,55 kilo mindre epler. I bedrifts&#xf8;konomien betyr dette at priselastisiteten<a id="noteref_3" class="noteref" epub:type="noteref" href="501325-32-footnotes.xhtml#fn_226_1">1</a> er p&#xe5; -1,55. Helsedirektoratet &#xf8;nsker at befolkningen skal spise mer frukt, og deres beregninger<a id="noteref_4" class="noteref" epub:type="noteref" href="501325-32-footnotes.xhtml#fn_226_2">2</a> viser at dersom man kutter momsen p&#xe5; frukt (23 %), vil salget &#xf8;ke med 10 %. Priselastisiteten p&#xe5; frukt er derfor p&#xe5; (10 %/(-23 %)) -0,43. Vi &#xf8;nsker derfor &#xe5; teste om koeffisienten for priselastisiteten p&#xe5; epler er lik -0,43 (legg merke til at vi n&#xe5; kj&#xf8;rer en tohalet test).</p><p>Det vi gj&#xf8;r her er med andre ord &#xe5; teste opp mot en bestemt verdi p&#xe5; <em>&#x3b2;.</em> Dette setter f&#xf8;ringer for nullhypotesen, og vi har n&#xe5; f&#xf8;lgende test vi skal utf&#xf8;re:</p><div id="page-227" class="page-normal" epub:type="pagebreak" title="227"></div><p><em>H</em><sub>0</sub>: <span class="asciimath">`beta = -0,43`</span> mot <em>H</em><sub>1</sub>: <em>&#x3b2;</em> &#x2260; -0,43</p><p>Selve testen gjennomf&#xf8;res akkurat som ovenfor. Den eneste forskjellen er at <em>t</em> beregnes p&#xe5; f&#xf8;lgende m&#xe5;te:</p><p><span class="asciimath">`t = (stackrel(^^)beta -b)/(s_(stackrel(^^)beta)) = (-1,55 -(- 0,43))/(0,338) = -3,31`</span></p><p><em>H</em><sub>0</sub> forkastes dersom |<em>t/ &gt; t<sub>a</sub></em>, det betyr |3,31| &gt; 2,306. Vi kan med andre ord forkaste <em>H</em><sub>0</sub>. <em>Konklusjonen</em> er derved at priselastisiteten for epler p&#xe5; -1,55 er utenfor priselastisiteten p&#xe5; frukt, som er p&#xe5; -0,43.</p></section></section><section id="level3_62"><h2 id="h3_62">10.5 Multippel regresjonsanalyse</h2><p>N&#xe5;r vi utvider den <em>enkle regresjonsmodellen</em> til <em>en multippel regresjonsmodell</em> betyr det i korte trekk at vi har flere enn &#xe9;n uavhengig variabel. Innledningsvis s&#xe5; vi p&#xe5; et eksempel hvor vi antok at <em>Omsetning</em> var en funksjon av variabler som beleggsprosent, pris, reklame, konkurranse og service.</p><p>Dette har vi tegnet opp grafisk i figur 10.5 og vi kan formelt skrive hypotesene p&#xe5; denne m&#xe5;ten:</p><figure id="imggroup_110" class="image"><img id="img_110" src="images/image110.jpg" alt="image" /><figcaption id="caption_107">Figur 10.5 Forskningsmodell over &#xe5;rsaker til omsetning p&#xe5; hotell.</figcaption></figure><p>H<sub>1</sub>: Beleggsprosent har en positiv effekt p&#xe5; omsetning</p><p>H<sub>2</sub>: Pris har en negativ effekt p&#xe5; omsetning</p><p>H<sub>3</sub>: Reklame har en positiv effekt p&#xe5; omsetning</p><p>H<sub>4</sub>: Konkurranse har en negativ effekt p&#xe5; omsetning</p><p>H<sub>5</sub>: Service har en positiv effekt p&#xe5; omsetning</p><p>Med andre ord:</p><p><em>Omsetning</em> = <em>F</em>(beleggsprosent, pris, reklame, konkurranse, service ...)</p><div id="page-228" class="page-normal" epub:type="pagebreak" title="228"></div><p>Dersom vi antar at funksjonen <em>F</em> er line&#xe6;r, kan vi, som vist tidligere, uttrykke denne sammenhengen mer presist ved f&#xf8;lgende ligning:</p><p><em>Omsetning</em> = <em>&#x3b2;</em><sub>0</sub> + <em>&#x3b2;</em><sub>1</sub><em>Belegg</em> + <em>&#x3b2;</em><sub>2</sub><em>Pris</em> + <em>&#x3b2;</em><sub>3</sub><em>Reklame</em> + <em>&#x3b2;</em><sub>4</sub><em>Konkurranse</em> + <em>&#x3b2;</em><sub>5</sub><strong><em>S</em></strong><em>ervice</em> +<em>&#x3b5;</em></p><p>Helt generelt vil en multippel regresjonsmodell v&#xe6;re uttrykt ved ligningen</p><p><em>Y<sub>i</sub></em> = <em>&#x3b2;</em><sub>0</sub> +<em>&#x3b2;</em><sub>1</sub><em>X</em><sub>1<em>i</em></sub> +<em>&#x3b2;</em><sub>2</sub> <em>X</em><sub>2<em>i</em></sub> +<em>&#x3b2;</em><sub>3</sub> <em>X</em><sub>3<em>i</em></sub> +...+<em>&#x3b2;<sub>k</sub> X<sub>ki</sub></em> +<em>&#x3b5;<sub>i</sub></em></p><p>hvor <em>Y</em> er den avhengige variabelen og <em>X</em>-ene representerer de <em>k</em>-uavhengige variablene, <em>&#x3b5;</em><sub>i</sub> er feilleddet, og <em>i-ene</em> refererer til den <em>i</em>-te observasjonen. <em>&#x3b2;</em><sub>0</sub> kalles konstantleddet, <em>&#x3b2;<sub>i</sub></em> (<em>i</em> = 1, 2, 3, ... , <em>k</em>) kalles regresjonsparametrene. En multippel regresjonsanalyse gjennomf&#xf8;res p&#xe5; samme m&#xe5;te som en enkel regresjonsanalyse. Vi kj&#xf8;rer regresjonsanalysen p&#xe5; vanlig m&#xe5;te, se figur 10.6:</p><figure id="imggroup_111" class="image"><img id="img_111" src="images/image111.jpg" alt="image" /><figcaption id="caption_108">Figur 10.6 Dialogboks for multippel regresjon.</figcaption></figure><p><em>Analyze &gt; Fit model</em></p><p>Vi skal bruke utskriften i tabell 10.3 fra denne analysen som eksempel p&#xe5; multippel regresjonsanalyse. Vi starter med &#xe5; se p&#xe5; de grafiske plottene fra hver av variablene. For det f&#xf8;rste ser vi at beleggsprosent ser ut til &#xe5; ha en positiv p&#xe5;virkning p&#xe5; omsetning, mens prisen har en negativ p&#xe5;virkning. Reklame ser ut til &#xe5; ha en positiv effekt, mens konkurranseintensitet ikke ser ut til &#xe5; ha noen p&#xe5;virkning overhodet, da regresjonslinjen er nesten helt vannrett, noe som inneb&#xe6;rer <span id="page-229" class="page-normal" epub:type="pagebreak" title="229"></span>at regresjonskoeffisienten er tiln&#xe6;rmet lik null. Dette f&#xe5;r vi bekreftet ved at sig-nifikansverdien er p&#xe5; 0,67, mens den b&#xf8;r v&#xe6;re lavere enn 0,10 for enhalet test. For effekten av service ser vi at heller ikke denne ser ut til &#xe5; ha noen signifikant effekt p&#xe5; omsetning.</p><p>Legg merke til i teksten for disse plottene er det poengtert at de er &#xab;leverage&#xbb;. Leverage-plottene i multippel regresjon gj&#xf8;r det mulig &#xe5; visuelt vise bidraget fra hver variabel. Dette betyr i korte trekk at helningsgraden p&#xe5; regresjonslinjen (regresjonskoeffisienten) kun kan tolkes i sammenheng med den modellen og de uavhengige variablene som man har inkludert i analysen.</p><figure id="imggroup_112" class="image"><img id="img_112" src="images/image112.jpg" alt="image" /><figcaption id="caption_109">Tabell 10.3 Utskrift fra multippel regresjonsanalyse i JMP.</figcaption></figure><figure id="imggroup_113" class="image"><img id="img_113" src="images/image113.jpg" alt="image" /><figcaption id="caption_110">Tabell 10.4 Forklaringskraften til den multiple regresjonsanalysen.</figcaption></figure></section><section id="level3_63"><div id="page-230" class="page-normal" epub:type="pagebreak" title="230"></div><h2 id="h3_63">I de p&#xe5;f&#xf8;lgende avsnittene skal vi tolke denne regresjonsutskriften mer inng&#xe5;ende.</h2><section id="level4_77"><h3 id="h4_77">10.6 Regresjonsligningens forklaringskraft, R<sup>2</sup></h3><p>N&#xe5; skal vi fors&#xf8;ke &#xe5; besvare f&#xf8;lgende sp&#xf8;rsm&#xe5;l: hvor godt er v&#xe5;r regresjonslig-ning tilpasset det observerte datamaterialet?</p><p>For &#xe5; regne ut regresjonsligningens forklaringskraft snakker vi ord om tre begreper; total variasjon, forklart variasjon og uforklart variasjon. Utledning for ligningene som man bruker for &#xe5; beregne disse tallene, finnes bakerst i kapitlet. Oppsummert kan vi si at regresjonsligningens forklaringskraft beregnes ut fra f&#xf8;lgende:</p><p>Total variasjon= Forklart variasjon+ Uforklart variasjon</p><p>TSS (total sum of squares)= RSS (regression sum of squares)+ ESS (error sum of squares)</p><p>Br&#xf8;ken <span class="asciimath">`(RSS)/(TSS)`</span> kalles andelen forklart variasjon</p><p>Et annet navn p&#xe5; dette forholdstallet er <em>determinasjonskoeffisienten,</em> og symbolet som brukes, er <em>R<sup>2</sup>.</em> Derfor kan vi skrive:</p><p><span class="asciimath">`R^2 = (RSS)/(TSS) iff R^2 = 1 - (ESS)/(TSS)`</span></p><p>Denne verdien benyttes som et m&#xe5;l for regresjonsligningens forklaringskraft. <em>R<sup>2</sup></em> vil ha en verdi mellom 0 og 1, og jo n&#xe6;rmere <em>R<sup>2</sup></em> er 1, jo bedre er forklaringskraften. N&#xe5;r <em>R<sup>2</sup></em> for eksempel er under 0,5 betyr dette atmerenn50 % av variasjonen i den avhengige variabelen ikke blir <em>forklart</em> av <span class="asciimath">`stackrel(^^)beta_0 + stackrel(^^)betaX_i`</span>, men er &#xe5; finne <em>utenfor</em> v&#xe5;r modell. Dette betyr alts&#xe5; at faktorer som vi ikke har tatt med, forklarer mye av variasjonen i den avhengige variabelen. I slike situasjoner vil vi fors&#xf8;ke &#xe5; utvide modellen ved &#xe5; trekke inn flere <em>forklaringsvariabler</em> (flere uavhengige variabler). Da er vi imidlertid over p&#xe5; temaet <em>multippel regresjon,</em> som blir behandlet i neste kapittel.</p><p>Under den enkle regresjonsanalysen fant vi at beleggsprosent forklarte 11,5 % av variasjonen i omsetningen p&#xe5; hotellet. I v&#xe5;r nye modell hvor vi har med fem forklaringsvariabler, ser vi at den forklarte variansen har &#xf8;kt til 42,6 %. Dette finner vi ved hjelp av br&#xf8;ken:</p><p><span class="asciimath">`R^2 = (RSS)/(TSS) iff (17335,638)/(40703,354) = 0,425 =42,6 %`</span></p><div id="page-231" class="page-normal" epub:type="pagebreak" title="231"></div><p><em>R</em><sup>2</sup> er imidlertid en ikke-avtakende funksjon av antall forklaringsvariabler. Dette betyr at jo flere forklaringsvariabler man inkluderer, jo h&#xf8;yere vil <em>R</em><sup>2</sup> bli. Dessverre er det slik at <em>R</em><sup>2</sup> vil &#xf8;ke ogs&#xe5; n&#xe5;r variabler som <em>ikke</em> er relevante, tas med som forklaringsvariabler. Det kan resultere i at vi kan f&#xe5; et <em>for optimistisk</em> inntrykk av regresjonsmodellens kvaliteter dersom<em>R</em><sup>2</sup> brukes ukritisk som kvalitetskriterium. Hvis man sammenligner to regresjonsmodeller med samme avhengige variabel, men med forskjellig antall forklaringsvariabler, skal man v&#xe6;re forsiktig med automatisk &#xe5; velge den modellen som har st&#xf8;rst <em>R</em><sup>2</sup>, se tabell 10.4 Skal man sammenligne to modeller p&#xe5; denne m&#xe5;ten, b&#xf8;r man ogs&#xe5; ta hensyn til &#xab;enkelhets-kriteriet&#xbb; (&#xab;simple is beautiful&#xbb;). Det vil si at man vurderer antallet <em>X</em>-variabler som er benyttet. En modell med f&#xe5; <em>X</em>-variabler er ofte mer generell enn en modell med mange <em>X</em>-variabler og kan av den grunn v&#xe6;re &#xe5; foretrekke. Dette elementet vil ikke komme fram i <em>R</em><sup>2</sup>. Forklaringskraften b&#xf8;r sees i sammenheng med antall <em>frihetsgrader</em> (<em>n</em>&#x2013;<em>k</em>&#x2013;<em>1</em>). <span class="asciimath">`barR^2`</span> (les: <em>R</em><sup>2</sup> - justert). Dette gj&#xf8;r vi ved &#xe5; inkludere antall frihetsgrader inn i formelen for <em>R</em><sup>2</sup>. Legg merke til at vi n&#xe5; tar utgangpunkt i uforklart varians over br&#xf8;kstreken.</p><p><span class="asciimath">`barR^2 = ((ESS)/((n-k-1)))/((TSS)/((n-1))) hArr 1 - ((23367,715)/(99-5-1))/((40703354)/((99-1))) = 39,5%`</span></p><p>I situasjoner hvor utvalgsst&#xf8;rrelsen er betydelig st&#xf8;rre enn antall forklaringsva-riabler, er det likegyldig hvilken av de to, <em>R</em><sup>2</sup> eller <em>R</em><sup>2</sup> man benytter. Er derimot utvalgsst&#xf8;rrelsen liten relativt til antall forklaringsvariabler, kan <em>R</em><sup>2</sup> og avvike betydelig. Da b&#xf8;r man feste mest lit til <em>R</em><sup>2</sup></p><p>EKSEMPEL 10.5</p><p>I dette eksemplet skal vi f&#xf8;lge opp eksemplet med epler og studere forklaringskraften i den estimerte sammenhengen mellom pris og salg av epler, <em>Y</em> (salg) og <em>X</em> (pris), for tallene oppgitt i tabell 10.2.</p><p>Estimert ligning:</p><p><span class="asciimath">`stackrel(^^)Salg_i = 50,3 - 1,55 Pris_i`</span></p><p>Etter litt regning kommer vi fram til at <em>RSS</em> = 323,493 <em>TSS</em> = 446,900 og <em>ESS</em> = 123,406.</p><p>Da blir:</p><p><span class="asciimath">`R^2 = (RSS)/(TSS) = (323,493)/(446,900) = 0,7238=72,38%`</span></p><div id="page-232" class="page-normal" epub:type="pagebreak" title="232"></div><p>Dette betyr at av den totale variasjonen i <em>Y</em> forklares med 72 % fra regresjons-ligningen 50,3 - 1,55 &#x2022; <em>X.</em> Vi ser ogs&#xe5; at 28 % er uforklart variasjon som m&#xe5; tilskrives faktorer som ligger utenfor modellen. Det ser derved ut til at 72 % av salget av epler forklares ut fra prisen p&#xe5; eplene.</p></section></section><section id="level3_64"><h2 id="h3_64">10.7 Test av den multiple regresjonsmodellen</h2><p>I hotelleksemplet v&#xe5;rt fant vi at beleggsprosent forklarte 11,5 % av variasjonen i omsetning for hotellet. N&#xe5;r vi inkluderte flere forklaringsvariabler, &#xf8;kte den forklarte variansen til 42,6 %, mens justert forklart varians var p&#xe5; 38,4 %. Likeledes som for enkel regresjonsanalyse, bruker vi her F-testen for &#xe5; finne ut om det er noen sammenheng mellom de valgte forklaringsvariablene og den avhengige variabelen.</p><section id="level4_78"><h3 id="h4_78">F-testen for multippel regresjonsmodell</h3><p>F-testen er en test av hele modellen. F-testen under multippel regresjonsanalyse er den samme som for enkel regresjonsanalyse. Vi tester p&#xe5; 5 %-niv&#xe5; <em>(&#x3b1;=</em> 0,05). Testen er som tidligere, bortsett fra at n&#xe5; har man trukket inn antall <em>k</em> (forklaringsvariabler) i testobservatoren:</p><p>H<sub>0</sub>: <em>R</em><sup>2</sup> = 0 mot H<sub>1</sub>: <em>R</em><sup>2</sup> &gt; 0</p><p><span class="asciimath">`F = ((RSS)/ k)/((ESS)/((n- k -1)))=((17335,638)/5)/((23367,715)/((99-5-1))) = 13,80`</span></p><p>Nullhypotesen forkastes dersom <em>F</em> &gt; <em>F</em><sub>k-1,n-k,&#x3b1;</sub>. Den kritiske <em>F</em>-verdien finner vi i tabellen bak i boka, (sl&#xe5;r opp verdiene 5 &#x2013; 1 = 4 og 99 &#x2013; 5 = 96 og <em>&#x3b1;</em> = 5 %), og finner verdien 3,65. Fra JMP-utskriften kan vi ogs&#xe5; lese av direkte at F-verdien er statistisk signifikant, s&#xe5; det er strengt tatt ikke n&#xf8;dvendig &#xe5; sl&#xe5; opp i tabellen.</p><p>Vi finner at <em>F</em>-verdien er h&#xf8;yere enn kritisk verdi 13,80 &gt; 3,65, og vi forkaster nullhypotesen om at modellen ikke har noen forklaringskraft. Konklusjonen er at det er en sammenheng mellom forklaringsvariablene og den avhengige variabelen.</p><figure id="imggroup_114" class="image"><img id="img_114" src="images/image114.jpg" alt="image" /><figcaption id="caption_111">Tabell 10.5 F-testen i JMP.</figcaption></figure></section><section id="level4_79"><div id="page-233" class="page-normal" epub:type="pagebreak" title="233"></div><h3 id="h4_79">t-testen for multippel regresjonsmodell</h3><p>N&#xe5;r vi skal teste om de enkelte <em>X</em>-variablene individuelt har en signifikant effekt p&#xe5; den avhengige variabelen, gj&#xf8;r vi dette p&#xe5; samme m&#xe5;te som i enkel regresjon. I v&#xe5;rt eksempel handler det om om beleggsprosent, pris, reklame, konkurranse og service har en effekt p&#xe5; omsetning ved hoteller. Dette gj&#xf8;r vi p&#xe5; samme m&#xe5;te som i enkel regresjonsanalyse, eneste forandringen er at antall frihetsgrader n&#xe5; er lik <em>n-k-1</em>, hvor <em>k</em> er antall forklaringsvariabler. Vi skal n&#xe5; teste regresjons-modellen med fem <em>X</em>-variabler basert p&#xe5; 101 observasjoner:</p><p><em>Y<sub>i</sub></em>=<em>&#x3b2;</em><sub>0</sub>+<em>&#x3b2;</em><sub>1</sub><em>X</em><sub>1</sub>+(-)<em>&#x3b2;</em><sub>2</sub><em>X</em><sub>2</sub>+<em>&#x3b2;</em><sub>3</sub><em>X</em><sub>3</sub>+(-)<em>&#x3b2;</em><sub>4</sub><em>X</em><sub>4</sub>+<em>&#x3b2;</em><sub>5</sub><em>X</em><sub>5</sub>+<em>&#x3b5;<sub>i</sub></em></p><p>Formelt vil hypoteseoppsettet se slik ut:</p><p><em>H</em><sub>0</sub>: <em>&#x3b2;</em><sub>1</sub> = 0 mot <em>H</em><sub>1</sub>: <em>&#x3b2;</em><sub>1</sub> &gt; 0</p><p><em>H</em><sub>0</sub>: <em>&#x3b2;</em><sub>2</sub> = 0 mot <em>H</em><sub>2</sub>: <em>&#x3b2;</em><sub>2</sub>  &lt;  0</p><p><em>H</em><sub>0</sub>: <em>&#x3b2;</em><sub>3</sub> = 0 mot <em>H</em><sub>3</sub>: <em>&#x3b2;</em><sub>3</sub> &gt; 0</p><p><em>H</em><sub>0</sub>: <em>&#x3b2;</em><sub>4</sub> = 0 mot <em>H</em><sub>4</sub>: <em>&#x3b2;</em><sub>4</sub>  &lt;  0</p><p><em>H</em><sub>0</sub>: <em>&#x3b2;</em><sub>5</sub> = 0 mot <em>H</em><sub>5</sub>: <em>&#x3b2;</em><sub>5</sub> &gt; 0</p><p>Dette betyr at vi tester at alternativhypotesen er ulik null (enten positiv eller negativ avhengig av prediksjonen). Vi velger et signifikansniv&#xe5; p&#xe5; 5 %, og har f&#xf8;lgende regresjonsestimater (se tabell 10.6):</p><table id="table_30"><tbody><tr><td colspan="6" rowspan="1"><strong>Parameter Estimates</strong></td></tr><tr><td rowspan="1" colspan="1"><strong>Term</strong></td><td rowspan="1" colspan="1"><strong>Estimate</strong></td><td rowspan="1" colspan="1"><strong>Std Error</strong></td><td rowspan="1" colspan="1"><strong>t Ratio</strong></td><td rowspan="1" colspan="1"><strong>Prob&gt;[t]</strong></td><td rowspan="1" colspan="1"><strong>Std Beta</strong></td></tr><tr><td rowspan="1" colspan="1">Intercept</td><td rowspan="1" colspan="1">18,449621</td><td rowspan="1" colspan="1">13,38944</td><td rowspan="1" colspan="1">1,38</td><td rowspan="1" colspan="1">0,1715</td><td rowspan="1" colspan="1">0</td></tr><tr><td rowspan="1" colspan="1">BELEGGPR</td><td rowspan="1" colspan="1">0,3605338</td><td rowspan="1" colspan="1">0,116077</td><td rowspan="1" colspan="1">3,08</td><td rowspan="1" colspan="1">0,0027*</td><td rowspan="1" colspan="1">0,245758</td></tr><tr><td rowspan="1" colspan="1">PRIS</td><td rowspan="1" colspan="1">-7.0B4729</td><td rowspan="1" colspan="1">1,174505</td><td rowspan="1" colspan="1">-6,03</td><td rowspan="1" colspan="1"> &lt; ,0001*</td><td rowspan="1" colspan="1">-0,51983</td></tr><tr><td rowspan="1" colspan="1">REKLAME</td><td rowspan="1" colspan="1">3,2007782</td><td rowspan="1" colspan="1">1,481276</td><td rowspan="1" colspan="1">2,17</td><td rowspan="1" colspan="1">0,0328*</td><td rowspan="1" colspan="1">0,179228</td></tr><tr><td rowspan="1" colspan="1">KONKURRANSE</td><td rowspan="1" colspan="1">-0,678003</td><td rowspan="1" colspan="1">1,607401</td><td rowspan="1" colspan="1">-0,42</td><td rowspan="1" colspan="1">0,6742</td><td rowspan="1" colspan="1">-0,03686</td></tr><tr><td rowspan="1" colspan="1">SERVICE</td><td rowspan="1" colspan="1">0,4632164</td><td rowspan="1" colspan="1">1,652279</td><td rowspan="1" colspan="1">0,28</td><td rowspan="1" colspan="1">0,7798</td><td rowspan="1" colspan="1">0,02232</td></tr></tbody></table><p>Tabell 10.6 t-test for multippel regresjonsanalyse i JMP.</p><p>For beleggsprosent gjelder f&#xf8;lgende: <span class="asciimath">`stackrel^^beta_1 = 0,36,s_stackrel^^beta = 0,12`</span></p><p>Dette gir f&#xf8;lgende <em>t</em>-verdi for beleggsprosent: <span class="asciimath">`t = stackrel^^beta_i/s_stackrel^^beta = (0,36)/(0,12) = 3,08`</span></p><p>Som vi ser av tabell 10.6, kan vi ogs&#xe5; lese denne verdien rett ut fra JMP-utskriften. Vi kan ogs&#xe5; lese signifikansniv&#xe5;et rett fra utskriften, eller vi kan g&#xe5; veien <span id="page-234" class="page-normal" epub:type="pagebreak" title="234"></span>via t-fordelingen, som er lagt ved i boka. Her sl&#xe5;r vi opp p&#xe5; antall frihetsgrader p&#xe5; 93 (99&#x2013;5&#x2013;1). Kritisk <em>t</em>-verdi er 1,662, mens testobservatoren er 3,08. Ved 5 % signifikansniv&#xe5; kan vi derfor konkludere med at vi kan forkaste nullhypotesen.</p></section><section id="level4_80"><h3 id="h4_80">Standardisert beta</h3><p>Legg merke til at vi i tabell 10.6 har tatt med en ny kolonne helt til h&#xf8;yre. Dette er standardisert beta. Standardisert beta kan v&#xe6;re nyttig &#xe5; inkludere n&#xe5;r vi jobber med multippel regresjon. Verdiene har vi f&#xe5;tt fram ved &#xe5; h&#xf8;yreklikke med musepila n&#xe5;r man st&#xe5;r over denne delen av utskriften og velger</p><p><em>Columns &gt; Std Beta.</em></p><p>Standardisert beta brukes n&#xe5;r man &#xf8;nsker &#xe5; sammenligne flere uavhengige variabler opp mot hverandre, og n&#xe5;r disse er m&#xe5;lt ved hjelp av ulik skala. Standardiseringen av betakoeffisientene betyr at verdien n&#xe5; vil g&#xe5; fra &#x2013;1 til 1. I v&#xe5;r utskrift kan vi ved hjelp av den standardiserte betakoeffisienten se at pris har den sterkeste p&#xe5;virkningen (&#x2013;0,52) og at denne er negativ, etterfulgt av beleggsprosent (0,25) som er positiv, og deretter reklame (0,18). Konkurranse og service var ikke signifikante og blir derfor ikke vurdert.</p><p><em>Konklusjonen</em> er derved at vi f&#xe5;r st&#xf8;tte for at hypotese 1, 2 og 3 beholdes, mens vi ikke f&#xe5;r st&#xf8;tte for hypotese 4 og 5. Analysene viser derved at beleggsprosent og reklame omsetningen p&#xe5; hotellene, mens pris reduserer omsetningen p&#xe5; hotellene. Konkurranse og service har ingen signifikant effekt p&#xe5; omsetningen p&#xe5; hotellene.</p></section></section><section id="level3_65"><h2 id="h3_65">10.8 For mange eller for f&#xe5; forklaringsvariabler</h2><p>Sp&#xf8;rsm&#xe5;let er deretter om vi har riktig antall forklaringsvariabler eller ikke. Kanskje skal <em>antall ansatte</em> v&#xe6;re med i modellen. Dette er fordi store hoteller med mange ansatte har st&#xf8;rre omsetning enn sm&#xe5; hoteller med f&#xe5; ansatte.</p><p>Feilspesifisering eller spesifikasjonsfeil knyttes i f&#xf8;rste rekke til feilaktig valg av forklaringsvariabler og feilaktig valg av funksjonsform. I denne framstillingen vil vi rette oppmerksomheten mot det f&#xf8;rste forholdet. Valg av forskjellige funksjonssammenhenger vil til en viss grad bli behandlet senere.</p><p>N&#xe5;r man velger forklaringsvariabler for en line&#xe6;r regresjonsmodell, er det viktig at bare relevante forklaringsvariabler er med. Dette er imidlertid vanskelig &#xe5; f&#xe5; til da man:</p><div id="page-235" class="page-normal" epub:type="pagebreak" title="235"></div><ul id="list_97"><li id="li_397">ikke vet med sikkerhet hvilke variabler som er relevante</li><li id="li_398">har et begrenset datasett, og det viser seg umulig &#xe5; f&#xe5; fatt i alle variabler man skulle &#xf8;nske seg</li><li id="li_399">kanskje arbeider med <em>proxyvariabler</em> (erstatningsvariabler) og de kan v&#xe6;re d&#xe5;rlige indikatorer for den virkelige variabelen</li></ul><p>Konsekvensene av &#xe5; ha med for mange forklaringsvariabler eller &#xe5; ha med for f&#xe5; er imidlertid kjent. Det er opplagt et mindre problem &#xe5; ha med for mange variabler. Det er snarere &#xe5; betrakte som et luksusproblem: Man kan alltid kvitte seg med det man har for mye av. Men det er ikke alltid like lett &#xe5; vite hvilke variabler man b&#xf8;r kvitte seg med. Har man derimot for f&#xe5; variabler (dvs. det er relevante variabler som er utelatt), er problemet permanent hvis det ikke er mulig &#xe5; skaffe til veie de &#xf8;nskede variablene.</p><section id="level4_81"><h3 id="h4_81">Konsekvens av &#xab;for lang&#xbb; modell</h3><p>La oss anta at <span class="asciimath">`Y_i = beta_0`</span> <span class="asciimath">`+ beta_`</span><span class="asciimath">`1X_1`</span> <span class="asciimath">`+ in_`</span><span class="asciimath">`i`</span> er den <em>sanne regresjonsmodellen.</em> Vi kan tenke oss at vi, mot bedre vitende, har valgt &#xe5; inkludere den <em>irrelevante</em> variabelen <em>X<sub>2</sub></em> og spesifisert f&#xf8;lgende regresjonsmodell:</p><p><span class="asciimath">`Y_i = beta_0 + beta_1X_1 + epsilon_i`</span></p><p>I de tilfellene hvor <em>X<sub>1</sub></em> og <em>X<sub>2</sub></em> er korrelert med hverandre, vil inkluderingen av <em>X<sub>2</sub></em> f&#xf8;re til at variansen i regresjonskoeffisienten for <em>X</em><sub>1</sub> blir p&#xe5;virket. Dette betyr at dersom vi har med <em>irrelevante</em> variabler som er korrelert med <em>relevante</em> variabler, vil presisjonen i regresjonen bli redusert. Dette gjelder selvsagt ogs&#xe5; for en generell regresjonsmodell hvor <em>k &gt; 2.</em></p></section><section id="level4_82"><h3 id="h4_82">Konsekvens av &#xab;for kort&#xbb; modell</h3><p>La oss n&#xe5; anta at <span class="asciimath">`Y_i = beta_ 0 + beta_ 1X_1 + beta_ 2X_2 + in_i`</span> er den <em>sanne regresjonsmodellen.</em> N&#xe5; tenker vi oss at vi har &#xab;glemt&#xbb; &#xe5; inkludere <em>X<sub>2</sub>.</em> Vi har med andre ord spesifisert f&#xf8;lgende regresjonsmodell:</p><p><span class="asciimath">`Y_i`</span> <span class="asciimath">`= beta_`</span><span class="asciimath">`0`</span> <span class="asciimath">`+ beta_`</span><span class="asciimath">`1X_1`</span> <span class="asciimath">`+ beta_`</span><span class="asciimath">`2X_2 + in_i`</span></p><p>Hvis vi som ovenfor antar at <em>X<sub>1</sub></em> og <em>X<sub>2</sub></em> er korrelerte, vil regresjonsestimatet ikke v&#xe6;re forventningsrettet: <span class="asciimath">`stackrel^^beta != beta_1`</span>. Dersom det <em>ikke</em> eksisterer en korrelasjon mellom<em>X</em><sub>1</sub> og <em>X<sub>2</sub></em> vil estimatet v&#xe6;re forventningsrettet. Skjevheten for parameterestimatene <span id="page-236" class="page-normal" epub:type="pagebreak" title="236"></span>er derved en funksjon av korrelasjonen mellom de <em>inkluderte</em> og de <em>ekskluderte</em> variablene. Dette f&#xf8;rer ogs&#xe5; til at variansen til den <em>feilspesifiserte</em> modellen vil v&#xe6;re st&#xf8;rre enn den estimerte variansen til <span class="asciimath">`in`</span>.</p><p>For korte regresjonsmodeller f&#xf8;rer alts&#xe5; til at:</p><ol id="list_98" class="list-style-type-none"><li id="li_400">1 de estimerte parametrene blir skjeve (ikke forventningsrette),</li><li id="li_401">2 variansen i feilleddet vil bli feilaktig estimert, og f&#xf8;lgelig kan hypotesetester og konfidensintervall gi villedende konklusjoner.</li></ol><p>La oss teste modellen v&#xe5;r fra hotellet ved &#xe5; inkludere <em>antall ansatte</em> som uavhengig variabel. Resultatene er som f&#xf8;lger:</p><table id="table_31"><tbody><tr><td colspan="2" rowspan="1"><strong>Summary of Fit</strong></td></tr><tr><td rowspan="1" colspan="1">RSquare</td><td rowspan="1" colspan="1">0,874251</td></tr><tr><td rowspan="1" colspan="1">RSquare Adj</td><td rowspan="1" colspan="1">0,66605</td></tr><tr><td rowspan="1" colspan="1">Root Mean Square Error</td><td rowspan="1" colspan="1">7,458882</td></tr><tr><td rowspan="1" colspan="1">Mean of Response</td><td rowspan="1" colspan="1">20,91910</td></tr><tr><td rowspan="1" colspan="1">Observations (or Sum Wgts)</td><td rowspan="1" colspan="1">00</td></tr></tbody></table><table id="table_32"><tbody><tr><td colspan="5" rowspan="1"><strong>Analysis of Variance</strong></td></tr><tr><td rowspan="1" colspan="1"><strong>Source</strong></td><td rowspan="1" colspan="1"><strong>DF</strong></td><td rowspan="1" colspan="1"><strong>Sum of Squares</strong></td><td rowspan="1" colspan="1"><strong>Mean Square</strong></td><td rowspan="1" colspan="1"><strong>F Ratio</strong></td></tr><tr><td rowspan="1" colspan="1">Model</td><td rowspan="1" colspan="1">6</td><td rowspan="1" colspan="1">35584,040</td><td rowspan="1" colspan="1">5930,82</td><td rowspan="1" colspan="1">106,6025</td></tr><tr><td rowspan="1" colspan="1">Error</td><td rowspan="1" colspan="1">02</td><td rowspan="1" colspan="1">5118,413</td><td rowspan="1" colspan="1">55,63</td><td rowspan="1" colspan="1"><strong>Prob&gt; F</strong></td></tr><tr><td rowspan="1" colspan="1">C Total</td><td rowspan="1" colspan="1">98</td><td rowspan="1" colspan="1">40703,354</td><td rowspan="1" colspan="1"></td><td rowspan="1" colspan="1"> &lt; ,0001*</td></tr></tbody></table><table id="table_33"><tbody><tr><td colspan="5" rowspan="1"><strong>Parameter Estimates</strong></td></tr><tr><td rowspan="1" colspan="1"><strong>Term</strong></td><td rowspan="1" colspan="1"><strong>Estimate</strong></td><td rowspan="1" colspan="1"><strong>Stil Error</strong></td><td rowspan="1" colspan="1"><strong>t Ratio</strong></td><td rowspan="1" colspan="1"><strong>Prob&gt;|t|</strong></td></tr><tr><td rowspan="1" colspan="1">Intercept</td><td rowspan="1" colspan="1">13,521082</td><td rowspan="1" colspan="1">6,366293</td><td rowspan="1" colspan="1">2,14</td><td rowspan="1" colspan="1">0,0347*</td></tr><tr><td rowspan="1" colspan="1">BELEGGPR</td><td rowspan="1" colspan="1">0,0031757</td><td rowspan="1" colspan="1">0,058474</td><td rowspan="1" colspan="1">0,05</td><td rowspan="1" colspan="1">0,9568</td></tr><tr><td rowspan="1" colspan="1">PRIS</td><td rowspan="1" colspan="1">-2,147034</td><td rowspan="1" colspan="1">0,616252</td><td rowspan="1" colspan="1">-3,48</td><td rowspan="1" colspan="1">0,0008*</td></tr><tr><td rowspan="1" colspan="1">REKLAME</td><td rowspan="1" colspan="1">-0,677773</td><td rowspan="1" colspan="1">0,729319</td><td rowspan="1" colspan="1">-0,93</td><td rowspan="1" colspan="1">0,3552</td></tr><tr><td rowspan="1" colspan="1">KONKURRANSE</td><td rowspan="1" colspan="1">0,2397671</td><td rowspan="1" colspan="1">0,758103</td><td rowspan="1" colspan="1">0,32</td><td rowspan="1" colspan="1">0,7525</td></tr><tr><td rowspan="1" colspan="1">SERVICE</td><td rowspan="1" colspan="1">-0,384899</td><td rowspan="1" colspan="1">0,778891</td><td rowspan="1" colspan="1">-0,49</td><td rowspan="1" colspan="1">0,6224</td></tr><tr><td rowspan="1" colspan="1">ANSATTE</td><td rowspan="1" colspan="1">6,5713277</td><td rowspan="1" colspan="1">0,031545</td><td rowspan="1" colspan="1">10,11</td><td rowspan="1" colspan="1"> &lt; ,6661*</td></tr></tbody></table><p>Tabell 10.7 Utskrift fra multippel regresjonsanalyse i JMP ved &#xe5; inkludere flere uavhengige variabler.</p><p>I tabell 10.7 ser vi hvordan resultatene blir dramatisk forandret n&#xe5;r vi inkluderer antall ansatte i regresjonsligningen. Ved &#xe5; inkludere st&#xf8;rrelse p&#xe5; bedriften gjennom <em>proxyvariabelen</em> antall ansatte ser vi at av de tidligere variablene er det n&#xe5; kun pris som har en signifikant effekt p&#xe5; omsetning. I tillegg er denne effekten negativ. Justert forklart varians er p&#xe5; hele 86,6 %, noe som betyr at vi har fanget opp mye av hva som p&#xe5;virker variasjonen i omsetning for hoteller. Dette illustrerer hvor viktig det er &#xe5; inkludere riktige variabler i modellen som testes.</p><p>Dersom vi n&#xe5; kj&#xf8;rer modellen med kun de signifikante sammenhengene, se tabell 10.8, ser vi at justert forklart varians er p&#xe5; hele 88,0 %. Vi ser ogs&#xe5; at forskjellen mellom <em>R</em><sup>2</sup> og <span class="asciimath">`barR^2`</span> er p&#xe5; kun 0,2 %. Dette er fordi man her har med f&#xe5; forklaringsvariabler (<em>k</em> = 2) i modellen. Legg merke til at antall frihetsgrader varierer i de ulike analysene av hotelldataene. Dette er fordi datasettet inneholder missing values.</p></section></section><section id="level3_66"><div id="page-237" class="page-normal" epub:type="pagebreak" title="237"></div><h2 id="h3_66">10.9 Dummy-regresjon</h2><p>I kapittel 6 l&#xe6;rte vi om nominalskala. If&#xf8;lge forutsetningsanalysen som er lagt medivedlegget til kapitlet kan man bruke slike kategorivariabler som uavhengige variabler, men ikke som avhengige. Eksempel p&#xe5; kategorivariabler er om hotellet i eksemplet over er medlem av en kjede eller ikke. Andre kategorivariabler er for eksempel &#xe5;rets fire sesonger: v&#xe5;r, sommer, h&#xf8;st og vinter. Kategorivariabler er dermed det laveste m&#xe5;leniv&#xe5;et man kan bruke.</p><figure id="imggroup_115" class="image"><img id="img_115" src="images/image115.jpg" alt="image" /><figcaption id="caption_112">Tabell 10.8 Utskrift fra multippel regresjonsanalyse n&#xe5;r man ekskluderer og inkluderer uavhengige variabler.</figcaption></figure><p>Det som skiller kategorivariablene fra de andre variablene, er at man ikke kan rangere kategorivariablene.</p><p>For eksempel kan man ikke si at v&#xe5;r er 3 ganger s&#xe5; mye som h&#xf8;st! Men vi m&#xe5; allikevel gi variablene en tallverdi for &#xe5; kunne bruke dem i JMP. Det vi sier er at vi <em>koder</em> kategoriene. Videre har vi ogs&#xe5; introdusertlik &#xab;avstand&#xbb; mellom kategoriene, men det er alts&#xe5; ingen grunn til at vi skal rangere eller snakke om avstand mellom kategoriene. N&#xe5;r man bruker kategorivariabler i regresjonsanalyse, kalles det <em>dummy-regresjon</em>. Vi skal n&#xe5; vise framgangsm&#xe5;ten for hvordan man kj&#xf8;rer slike analyser, samt hvordan man tolker resultatene. For &#xe5; kj&#xf8;re en dummy-regresjon m&#xe5; variabelen v&#xe6;re merket p&#xe5; <em>nominalniv&#xe5;</em> eller p&#xe5; <em>ordinalniv&#xe5;</em> i dataarket i JMP. La oss teste hvorvidt det er noen forskjell i omsetning p&#xe5; hoteller som er medlem av en kjede eller ikke. I JMP legger vi inn omsetning som avhengig variabel og kjede som uavhengig, se figur 10.7.</p><div id="page-238" class="page-normal" epub:type="pagebreak" title="238"></div><figure id="imggroup_116" class="image"><img id="img_116" src="images/image116.jpg" alt="image" /><figcaption id="caption_113">Figur 10.7 Dialogboks for dummy-regresjon.</figcaption></figure><p><em>Analyze  &lt;  Fit Model</em></p><p>I tabell 10.9 ser vi at den gjennomsnittlige omsetningen for de som ikke er i en kjede, er p&#xe5; 11,77 millioner kroner. For de som er i en kjede, er den gjennomsnittlige omsetningen p&#xe5; 22,05 millioner kroner.</p><figure id="imggroup_117" class="image"><img id="img_117" src="images/image117.jpg" alt="image" /><figcaption id="caption_114">Tabell 10.9 Dummy-regresjon i JMP n&#xe5;r uavhengige variabler er p&#xe5; kategoriniv&#xe5;.</figcaption></figure><div id="page-239" class="page-normal" epub:type="pagebreak" title="239"></div><p>Legg merke til at JMP tester om regresjonskoeffisientene er forskjellig fra null. For &#xe5; foreta en test om kategorivariablene er forskjellig fra hverandre (om omsetning blant kjedehotellene p&#xe5; 22 er signifikant forskjellig fra omsetning p&#xe5; ikke-kjede hotellene p&#xe5; 11,7). For &#xe5; f&#xe5; mer detaljert informasjon gj&#xf8;r vi f&#xf8;lgende i utskriften, se tabell 10.10. Vi klikker p&#xe5; <em>R&#xf8;d trekant</em> for kjedefiguren, og velger:</p><p><em>R&#xf8;d trekant</em> &gt; <em>LSMeans Plot og LSMeans Student's t R&#xf8;d trekant</em> &gt; <em>Detailed Comparison</em></p><figure id="imggroup_118" class="image"><img id="img_118" src="images/image118.jpg" alt="image" /><figcaption id="caption_115">Tabell 10.10 Dialogboks i JMP for &#xe5; teste signifikans mellom kategorier.</figcaption></figure><p>Vi f&#xe5;r n&#xe5; grafisk fram den gjennomsnittlige forskjellen, samt sannsynligheten for signifikant forskjell mellom disse kategoriene. Som vi ser fra utskriften, er det en signifikant forskjell i omsetning mellom kjedehotellene og de som ikke er medlem av en kjede, tabell 10.11.</p><div id="page-240" class="page-normal" epub:type="pagebreak" title="240"></div><figure id="imggroup_119" class="image"><img id="img_119" src="images/image119.jpg" alt="image" /><figcaption id="caption_116">Tabell 10.11 Utskrift fra dummy-regresjon i JMP.</figcaption></figure><p>I JMP kan man f&#xf8;lge denne prosedyren uansett hvor mange kategorier variablene er p&#xe5;. Det er ikke n&#xf8;dvendig &#xe5; rekode variablene til forskjellige kolonner, slik som man m&#xe5; i for eksempel SPSS. JMP vil automatisk legge den <em>siste</em> kategorien inn i konstantleddet, se tabell 10.12. Dette ser vi fra utskriften. Grunnen er at en inklusjon av den siste kategorien ville ha skapt line&#xe6;r avhengighet i parametrene fordi hver enkelt kategori kan uttrykkes som en funksjon av de andre kategoriene, og det er brudd mot reglene for &#xe5; kj&#xf8;re regresjonsanalyse.</p><table id="table_34"><tbody><tr><td colspan="5" rowspan="1"><strong>Parameter Estimates</strong></td></tr><tr><td rowspan="1" colspan="1"><strong>Term</strong></td><td rowspan="1" colspan="1"><strong>Estimate</strong></td><td rowspan="1" colspan="1"><strong>Std Error</strong></td><td rowspan="1" colspan="1"><strong>t Ratio</strong></td><td rowspan="1" colspan="1"><strong>Prob&gt;|t|</strong></td></tr><tr><td rowspan="1" colspan="1">Intercept</td><td rowspan="1" colspan="1">16,910731</td><td rowspan="1" colspan="1">1,58599</td><td rowspan="1" colspan="1">10,66</td><td rowspan="1" colspan="1"> &lt; ,0001*</td></tr><tr><td rowspan="1" colspan="1">KJEDE[Kjede]</td><td rowspan="1" colspan="1">5,1440639</td><td rowspan="1" colspan="1">1,58599</td><td rowspan="1" colspan="1">3,24</td><td rowspan="1" colspan="1">0,0015*</td></tr></tbody></table><p>Tabell 10.12 Betaverdier for dummy regresjon i JMP.</p><div id="page-241" class="page-normal" epub:type="pagebreak" title="241"></div><p>Det er viktig &#xe5; merke seg at JMP behandler variabler p&#xe5; ordinalniv&#xe5; som kategorier i regresjonsanalysen. Dette er statistisk riktig, og dersom man &#xf8;nsker &#xe5; overstyre denne forutsetningen, m&#xe5; man lage et brudd p&#xe5; den ved &#xe5; omdefinere variabelen til &#xe5; v&#xe6;re p&#xe5; kontinuerlig niv&#xe5;, se figur 10.8.</p><figure id="imggroup_120" class="image"><img id="img_120" src="images/image120.jpg" alt="image" /><figcaption id="caption_117">Figur 10.8 Dialogboks for &#xe5; endre skalaniv&#xe5; i JMP.</figcaption></figure><p>Vi kan ogs&#xe5; kombinere dummy-variabler i den ordin&#xe6;re regresjonsanalysen. I eksemplet under har vi tatt med variablene pris og sesong som uavhengige variabler, og omsetning som avhengig variabel, se tabell 10.13</p><figure id="imggroup_121" class="image"><img id="img_121" src="images/image121.jpg" alt="image" /><figcaption id="caption_118">Tabell 10.13 Utskrift fra dummy regresjonsanalyse n&#xe5;r man har med flere uavhengige variabler.</figcaption></figure><div id="page-242" class="page-normal" epub:type="pagebreak" title="242"></div><p>I tabell 10.13 ser vi at pris har en negativ effekt p&#xe5; omsetning &#x2013; dette visste vi jo fra tidligere. Samtidig ser vi at av de tre kategoriene har vintersesong h&#xf8;yest gjennomsnittlig omsetning, etterfulgt av sommersesong, og til slutt hoteller som driver i begge sesongene. Dette er en god illustrasjon p&#xe5; at kategorivariabler har en konstant verdi. Hadde vi g&#xe5;tt dypere inn i dette tallmaterialet ved &#xe5; f&#xf8;lge prosedyrene som er beskrevet i eksemplet ovenfor, ville vi ha funnet ut at det ikke er noen signifikant forskjell mellom disse tre kategoriene.</p></section><section id="level3_67"><h2 id="h3_67">10.10 Eksempel p&#xe5; multippel regresjonsanalyse ved hjelp av JMP</h2><p>Vi skal n&#xe5; g&#xe5; igjennom en eksamensoppgave fra Handelsh&#xf8;yskolen BI. Oppgaven finnes i vedlegget til boka. Den tar for seg et stort norsk helseforetak som sliter med turnover, det vil si at mange ansatte slutter i jobben. Gjennom en sp&#xf8;rreskjemaunders&#xf8;kelse har man m&#xe5;lt disse forklaringsfaktorene til turnover: mulighet for forfremmelse, l&#xf8;nnsforhold, forutsigbarhet med hensyn til arbeidstider samt stress og underbemanning.</p><figure id="imggroup_122" class="image"><img id="img_122" src="images/image122.jpg" alt="image" /><figcaption id="caption_119">Figur 10.10 Dialogboks for multippel regresjon i JMP.</figcaption></figure><p>Hypotesene som blir testet er f&#xf8;lgende:</p><p><em>H</em><sub>1</sub>: Jo mer negative arbeidstider, jo st&#xf8;rre sannsynlighet for &#xe5; slutte i jobben</p><p><em>H</em><sub>2</sub>: Jo mer jobbstress, jo st&#xf8;rre sannsynlighet for &#xe5; slutte i jobben</p><p><em>H</em><sub>3</sub>: Jo mer misn&#xf8;ye med l&#xf8;nnen, jo st&#xf8;rre sannsynlighet for &#xe5; slutte i jobben</p><p><em>H</em><sub>4</sub>: Jo st&#xf8;rre arbeidsbelastning, jo st&#xf8;rre sannsynlighet for &#xe5; slutte i jobben</p><p><em>H</em><sub>5</sub>: Jo st&#xf8;rre mulighet for forfremmelse, jo <em>mindre</em> sannsynlighet for &#xe5; slutte i jobben</p><div id="page-243" class="page-normal" epub:type="pagebreak" title="243"></div><p>Sp&#xf8;rreskjemaet er gjengitt under, slik at man ser hvordan variablene er operasjonalisert. NB: V&#xe6;r oppmerksom p&#xe5; at det med vilje er lagt inn svakheter i sp&#xf8;rreskjemaet som en del av eksamensoppgaven. I dette eksemplet er det imidlertid fokusert p&#xe5; selve regresjonsanalysen.</p><aside id="sidebar_31" class="sidebar" epub:type="sidebar">
<table id="table_35"><tbody><tr><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">Hvor stor sannsynlighet er det at du vil slutte i jobben hos Cure i l&#xf8;pet av det n&#xe6;rmeste &#xe5;ret?</td><td rowspan="1" colspan="1">Antall prosent</td><td rowspan="1" colspan="1">&#x25a1;</td><td rowspan="1" colspan="1"></td><td rowspan="1" colspan="1"></td><td rowspan="1" colspan="1"></td></tr><tr><td colspan="7" rowspan="1">Vennligst marker i hvilken grad du er enig/uenig med p&#xe5;standene:</td></tr><tr><td rowspan="1" colspan="1"></td><td rowspan="1" colspan="1"></td><td rowspan="1" colspan="1">Meget uenig</td><td rowspan="1" colspan="1">Uenig</td><td rowspan="1" colspan="1">Enig</td><td rowspan="1" colspan="1">Meget Enig</td><td rowspan="1" colspan="1">Sv&#xe6;rt Enig</td></tr><tr><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">Arbeidstidene her er ikke bra</td><td rowspan="1" colspan="1">&#x25a1;</td><td rowspan="1" colspan="1">&#x25a1;</td><td rowspan="1" colspan="1">&#x25a1;</td><td rowspan="1" colspan="1">&#x25a1;</td><td rowspan="1" colspan="1">&#x25a1;</td></tr><tr><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">Min jobb er sv&#xe6;rt stressende</td><td rowspan="1" colspan="1">&#x25a1;</td><td rowspan="1" colspan="1">&#x25a1;</td><td rowspan="1" colspan="1">&#x25a1;</td><td rowspan="1" colspan="1">&#x25a1;</td><td rowspan="1" colspan="1">&#x25a1;</td></tr><tr><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">Jeg er veldig misforn&#xf8;yd med min n&#xe5;v&#xe6;rende l&#xf8;nn</td><td rowspan="1" colspan="1">&#x25a1;</td><td rowspan="1" colspan="1">&#x25a1;</td><td rowspan="1" colspan="1">&#x25a1;</td><td rowspan="1" colspan="1">&#x25a1;</td><td rowspan="1" colspan="1">&#x25a1;</td></tr><tr><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">Det er stor underbemanning p&#xe5; min avdeling</td><td rowspan="1" colspan="1">&#x25a1;</td><td rowspan="1" colspan="1">&#x25a1;</td><td rowspan="1" colspan="1">&#x25a1;</td><td rowspan="1" colspan="1">&#x25a1;</td><td rowspan="1" colspan="1">&#x25a1;</td></tr><tr><td rowspan="1" colspan="1">6</td><td rowspan="1" colspan="1">Jeg mener jeg har store muligheter for forfremmelse i min n&#xe5;v&#xe6;rende jobb</td><td rowspan="1" colspan="1">&#x25a1;</td><td rowspan="1" colspan="1">&#x25a1;</td><td rowspan="1" colspan="1">&#x25a1;</td><td rowspan="1" colspan="1">&#x25a1;</td><td rowspan="1" colspan="1">&#x25a1;</td></tr><tr><td rowspan="1" colspan="1">7</td><td rowspan="1" colspan="1">Kj&#xf8;nn</td><td rowspan="1" colspan="1">Mann Kvinne</td><td rowspan="1" colspan="1">&#x25a1;</td><td rowspan="1" colspan="1">&#x25a1;</td><td rowspan="1" colspan="1"></td><td rowspan="1" colspan="1"></td></tr></tbody></table>
</aside><p>Figur 10.9 Sp&#xf8;rreskjema fra CURE-caset.</p><p>R&#xe5;dataene fra JMP er gjengitt i tabellen:</p><table id="table_36"><tbody><tr><td rowspan="1" colspan="1">30</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">kvinne</td></tr><tr><td rowspan="1" colspan="1">10</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">kvinne</td></tr><tr><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">kvinne</td></tr><tr><td rowspan="1" colspan="1">25</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">mann</td></tr><tr><td rowspan="1" colspan="1">80</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">kvinne</td></tr><tr><td rowspan="1" colspan="1">85</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">mann</td></tr><tr><td rowspan="1" colspan="1">60</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">mann</td></tr><tr><td rowspan="1" colspan="1">30</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">mann</td></tr><tr><td rowspan="1" colspan="1"><span id="page-244" class="page-normal" epub:type="pagebreak" title="244"></span>55</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">kvinne</td></tr><tr><td rowspan="1" colspan="1">45</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">kvinne</td></tr><tr><td rowspan="1" colspan="1">100</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">kvinne</td></tr><tr><td rowspan="1" colspan="1">36</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">mann</td></tr><tr><td rowspan="1" colspan="1">14</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">kvinne</td></tr><tr><td rowspan="1" colspan="1">66</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">mann</td></tr><tr><td rowspan="1" colspan="1">58</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">mann</td></tr><tr><td rowspan="1" colspan="1">32</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">mann</td></tr><tr><td rowspan="1" colspan="1">12</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">kvinne</td></tr><tr><td rowspan="1" colspan="1">16</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">kvinne</td></tr><tr><td rowspan="1" colspan="1">88</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">kvinne</td></tr><tr><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">kvinne</td></tr><tr><td rowspan="1" colspan="1">85</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">mann</td></tr><tr><td rowspan="1" colspan="1">75</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">mann</td></tr><tr><td rowspan="1" colspan="1">64</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">mann</td></tr><tr><td rowspan="1" colspan="1">50</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">kvinne</td></tr><tr><td rowspan="1" colspan="1">36</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">kvinne</td></tr></tbody></table><p>En multippel regresjonsanalyse i JMP fra de spesifiserte hypotesene gir f&#xf8;lgende: (se tabell 10.14)</p><figure id="imggroup_123" class="image"><img id="img_123" src="images/image123.jpg" alt="image" /><figcaption id="caption_120">Tabell 10.14 Utskrift fra multippel regresjon i Cure-caset.</figcaption></figure><div id="page-245" class="page-normal" epub:type="pagebreak" title="245"></div><p>Her ser man f&#xf8;rst at regresjonsligningen forklarer 77,5 % av variasjonen i sannsynligheten for &#xe5; slutte. Dette er bra. For F-testen ser vi at modellen er signifikant. Avviket mellom <em>R</em><sup>2</sup> og <span class="asciimath">`bar<em>R^2</em>`</span> er p&#xe5; henholdsvis 77,5 og 71,6. La oss s&#xe5; kj&#xf8;re den multiple regresjonsmodellen uten forfremmelse og l&#xf8;nn (se tabell 10.15).</p><figure id="imggroup_124" class="image"><img id="img_124" src="images/image124.jpg" alt="image" /><figcaption id="caption_121">Tabell 10.15 Utskrift fra multippel regresjon i Cure-caset for revidert modell.</figcaption></figure><p>Det f&#xf8;rstevi legger merke til, er at <em>R</em><sup>2</sup> ogn&#xe5;har mindre avvik, oger p&#xe5; 77,2 og73,9. Dette indikerer at modellen vi n&#xe5; jobber med, er mer realistisk. Dette kan man anta er fordi at kontrollerer for antall frihetsgrader, og straffer dermed store modeller.</p><p>De uavhengige variablene forklarer n&#xe5; 77,2 % av variasjonen i sannsynligheten for &#xe5; slutte. Ubeleilig arbeidstid har sterkest effekt i den gitte modellen, med standardisert regresjonskoeffisient p&#xe5; 0,47. Underbemanning f&#xf8;lger deretter, med standardisert regresjonskoeffisient p&#xe5; 0,29 og stress med 0,26. N&#xe5;r man g&#xe5;r over til de ustandardiserte regresjonskoeffisientene, kan man sammenligne hva endringene medf&#xf8;rer i praksis. Hvis den ansattes evaluering av ulempe med arbeidstiden &#xf8;ker med en enhet i forhold til et sp&#xf8;rreskjema p&#xe5; 1&#x2013;5, &#xf8;ker sannsynligheten for at hun/han vil slutte med 9,2 %. For stress er verdien p&#xe5; 5,6 % og for underbemanning 6,7 %.</p><div id="page-246" class="page-normal" epub:type="pagebreak" title="246"></div><p>Formelt er hypotese 1, 2 og 4 st&#xf8;ttet, mens hypotese 3 og 5 ikke er st&#xf8;ttet. De sistnevnte kan derfor ikke forkastes ut fra det rapporterte signifikansniv&#xe5;et uten stor sannsynlighet for &#xe5; foreta en Type I-feil. Den f&#xf8;rste multiple regresjonsanalysen viste oss at hvis vi forkastet nullhypotesen til l&#xf8;nn <span class="asciimath">`(1 &#x2013; (0,772//2) = 0,614)`</span> og forfremmelse <span class="asciimath">`(1 &#x2013; (0,627//2) = 0,686)`</span>, ville det ha v&#xe6;rt henholdsvis 61,4 % sannsynlighet for &#xe5; forkastet en sann nullhypotese for l&#xf8;nn og 68,6 % sannsynlighet for &#xe5; forkastet en sann nullhypotese for forfremmelse. Husk at vi har en enhalet test (hypotesene har spesifisert retning), og vi derfor delte signifikansniv&#xe5;et p&#xe5; to.</p><aside id="sidebar_32" class="sidebar" epub:type="sidebar">
<p><strong>10.11 Oppsummering</strong></p>
<p>Ved enkel regresjonsanalyse fors&#xf8;ker vi &#xe5; beskrive sammenhengen mellom en avhengig variabel, vanligvis kalt <em>Y,</em> og en uavhengig variabel (forklaringsvariabel), vanligvis kalt <em>X</em>. Siden vi antar at sammenhengen ikke er perfekt, m&#xe5; vi ogs&#xe5; inkludere et feilledd. Den matematiske modellen er gitt ved formelen:</p>
<p><span class="asciimath">`Y = beta_0 + betaX + in`</span></p>
<p>Regresjonsparametrene og feilleddet er ukjente st&#xf8;rrelser som vi &#xf8;nsker &#xe5; estimere. Metoden som benyttes, er minste kvadraters metode. Ideen er &#xe5; finne fram til parameterestimater som er slik at summen av de kvadrerte residualene (estimerte feilledd) blir minimal.</p>
<p>Straks parameterestimatene er p&#xe5; plass, m&#xe5; modellen underkastes statistiske tester. Det viktigste er &#xe5; teste om sammenhengen mellom <em>Y</em> og <em>X</em> er signifikant forskjellig fra null. Rent formelt setter vi opp en nullhypotese som vanligvis uttrykker at sammenhengen er lik null. Dersom denne forkastes, vil vi kunne p&#xe5;st&#xe5; at <em>det er en signifikant sammenheng</em> mellom <em>Y</em> og <em>X</em>. Testobservatoren, som beregnes som forholdet mellom parameterestimatet <span class="asciimath">`(barR^<sup>2</sup>)`</span> og den tilh&#xf8;rende standardfeilen <span class="asciimath">`(barR^<sup>2</sup>)`</span>, kan vises &#xe5; v&#xe6;re <em>t</em>-fordelt med antall frihetsgrader lik <em>n</em> &#x2013; 2.</p>
<p>Dersom vi kan konkludere med at nullhypotesen skal forkastes, kan det v&#xe6;re interessant &#xe5; bruke modellen til prediksjon. Det betyr at vi bruker de estimerte parametrene til &#xe5; beregne verdier for den avhengige variabelen (<em>Y</em>) for oppgitte verdier av <em>X</em>. Her kan vi ta hensyn til usikkerheten i parameterestimatene s&#xe5; vel som usikkerheten forbundet med feilleddet, ved &#xe5; beregne prediksjonsintervaller.</p>
<p><em>Multippel regresjonsanalyse</em> er en metode for &#xe5; beskrive sammenhengen <span id="page-247" class="page-normal" epub:type="pagebreak" title="247"></span>mellom en avhengig (stokastisk) variabel og et sett med uavhengige (vanligvis ikke-stokastiske) variabler. Som for den enkle regresjonsmodellen er hovedpoenget &#xe5; finne fram til parametre (regresjonsparametre, regresjonskoeffisienter) som uttrykker sammenhengen mellom de aktuelle uavhengige variablene og den avhengige variabelen. Modellen uttrykkes ved ligningen:</p>
<p><span class="asciimath">`Y_i =beta_0 +beta_1`</span><span class="asciimath">`X_(`</span><span class="asciimath">`1i) +beta_2`</span><span class="asciimath">`X_(`</span><span class="asciimath">`2i) +beta_3`</span><span class="asciimath">`X_(`</span><span class="asciimath">`3i) +...beta_kX_(ki) + in_i`</span></p>
<p>Parametrene estimeres ved minste kvadraters metode (<em>MKM</em>). Gauss-Markov-teoremet garanterer at <em>MKM</em> er &#xab;optimal&#xbb; (BLUE) dersom <em>de klassiske antakelsene</em> er oppfylt.</p>
<p>Sentralt i multippel regresjonsanalyse st&#xe5;r modellevaluering. Her vurderes modellens enkelte kvaliteter: forklaringskraft, signifikans og i hvilken grad de klassiske antakelsene er oppfylt.</p>
<p><em>Forklaringskraften</em> m&#xe5;les med <em>andelen av forklart variasjon</em> (<em>R</em><sup>2</sup> og <span class="asciimath">`barR^2`</span>) og b&#xf8;r v&#xe6;re s&#xe5; stor som overhodet mulig. Til en viss grad er den &#xe5; sammenligne med et p&#xe5;litelighetsm&#xe5;l. Det er imidlertid ingen garanti for at en har funnet den rette sammenhengen selv om forklaringskraften er stor.</p>
<p><em>Signifikans</em> handler om hvorvidt vi kan konkludere med at den aktuelle uavhengige variabelen har en signifikant <em>effekt</em> p&#xe5; den avhengige variabelen. Vanligvis testes dette ved t-tester for hver enkelt regresjonsparameter. Som for enkel regresjon er det slik at forholdet mellom estimert parameterverdi og estimert standardfeil <span class="asciimath">`stackrel^^beta/s_stackrel^^beta`</span> er <em>t</em>-fordelt. Antall frihetsgrader er <em>n</em>-<em>k-</em>1.</p>
<p>I vedlegget forklares forutsetningsanalyse for regresjon. i regrejson antas det (forutsettes) at de uavhengige variablene er line&#xe6;rt uavhengige. Dersom dette ikke er tilfellet, har vi et fenomen som kalles perfekt <em>kollinearitet</em>. Dette er s&#xe6;rdeles alvorlig, men vil sjelden eller aldri bli noe problem for oss i og med at &#xab;programvaren&#xbb; setter en stopper for det. Ikke-perfekt kollinearitet derimot, m&#xe5; vi kunne takle. Det handler om tilfeller hvor to eller flere uavhengige variabler er line&#xe6;rt avhengige <em>inntil en viss grad</em>. Dette kan for&#xe5;rsake upresise parameterestimater. Variansen til parameterestimatene blir uhensiktsmessig stor, og f&#xf8;lgelig kan vi ende opp med for sm&#xe5; <em>t</em>-verdier. Beregning av korrelasjoner mellom de uavhengige variablene og beregning av VIF-koeffisienten kan v&#xe6;re til hjelp for &#xe5; avdekke om kollineariteten er av en slik art at den kan skape problemer.</p>
<p>Et annet viktig punkt er i hvilken grad feilleddet oppf&#xf8;rer seg som det burde. I residualanalysen har vi konsentrert oss om normalitet og heteroskedastisitet. Heteroskedastisitet har vi dersom feilleddet ikke har konstant <span id="page-248" class="page-normal" epub:type="pagebreak" title="248"></span>varians. Det kan vi oppdage ved &#xe5; plotte residualet mot de uavhengige variablene og mot predikert <em>Y</em>. Skulle vi oppdage tydelige &#xab;m&#xf8;nstre&#xbb; som indikerer systematiske sammenhenger, har vi sannsynligvis heteroskedastisitet. For v&#xe6;re p&#xe5; den &#xab;sikre siden&#xbb; b&#xf8;r vi imidlertid gjennomf&#xf8;re en formell test.</p>
</aside></section><section id="level3_68"><div id="page-249" class="page-normal" epub:type="pagebreak" title="249"></div><h2 id="h3_68">VEDLEGG</h2><section id="level4_83"><h3 id="h4_83">Klassiske antakelser:</h3><p><em>Statistiske antakelser</em> om den line&#xe6;re regresjonsmodellen (klassiske antakelser):</p><ol id="list_99" class="list-style-type-none"><li id="li_402">1 Forstyrrelsesleddet (feilleddet) skal v&#xe6;re gjennomsnittlig lik 0.</li><li id="li_403">2 Frav&#xe6;r av heteroskedastisitet: Variansen til forstyrrelsesleddet skal v&#xe6;re konstant.</li><li id="li_404">3 Frav&#xe6;r av autokorrelasjon: Forstyrrelsesleddet fra en observasjon er uavhengig av (ukorrelert med) forstyrrelsesleddet fra en annen observasjon. Dette gjelder ved tidsserie-data.</li><li id="li_405">4 Forstyrrelsesleddet skal v&#xe6;re normalfordelt.</li><li id="li_406">5 Alle forklaringsvariabler er ukorrelerte med forstyrrelsesleddet.</li><li id="li_407">6 Ligningen skal v&#xe6;re line&#xe6;r i koeffisientene (parametrene).</li><li id="li_408">7 Frav&#xe6;r av multikollinearitet: Ingen forklaringsvariabel kan v&#xe6;re en line&#xe6;r kombinasjon av andre forklaringsvariabler. (Denne forutsetningen gjelder kun for multippel regresjon).</li></ol></section><section id="level4_84"><h3 id="h4_84">Gauss-Markov-teoremet</h3><p>Dersom forutsetningene gjelder for antakelse 1&#x2013;6, sies minste kvadraters metode &#xe5; v&#xe6;re BLUE. BLUE er en forkortelse for <strong>B</strong>est <strong>L</strong>inear <strong>U</strong>nbiased <strong>E</strong>stimator. Oversatt til norsk betyr det at <em>MKM</em> er den beste line&#xe6;re estimeringsmetoden som gir forventningsrette estimater. Dette er innholdet i <em>Gauss-Markov-teoremet.</em></p><p>At <em>MKM</em> gir de beste estimatene, er i betydningen <em>estimater med minimal varians:</em> La oss tenke oss at <span class="asciimath">`stackrel^^beta_(MKM)`</span> er estimert med <em>MKM,</em> og at <span class="asciimath">`stackrel^^beta_("ANNEN")`</span> er estimert med en hvilken som helst annen metode &#x2013; da sier Gauss-Markovteoremet at:</p><p><span class="asciimath">`Var(stackrel^^beta_(MKM))  &lt; = Var(stackrel^^beta_("ANNEN"))`</span></p><p>Forventningsrett handler om at <em>MKM</em>-estimatene i gjennomsnitt vil v&#xe6;re lik den sanne verdi. Hvis vi gj&#xf8;r gjentatte estimeringer av samme parameter, vil gjennomsnittet av disse estimatene v&#xe6;re lik den sanne verdi. Da sier vi at forventningsverdien til <span class="asciimath">`stackrel^^beta`</span> <span class="asciimath">`= beta`</span>.</p><p>Matematisk kan dette uttrykkes ved ligningen <span class="asciimath">`E(stackrel^^beta) = beta.`</span></p><div id="page-250" class="page-normal" epub:type="pagebreak" title="250"></div><p>For antakelse 7 gjelder det at det skal ikke eksistere parametre a<sub>1</sub>, a<sub>2</sub>, ... , a<sub>k</sub> slik at <span class="asciimath">`a_1X_1 + a_2X_2 + ... + a_kX_k = 0`</span>.</p></section><section id="level4_85"><h3 id="h4_85">Test av antakelse 2 &#x2013; residualanalyse</h3><p>Residualanalysen handler om analysen av feilleddet (forstyrrelsesleddet). Som vi s&#xe5; av de statistiske antakelsene for linear regresjonsanalyse, handlet mange om feilleddet. Slik er det fordi feilleddet spiller en sentral rolle i regresjonsanalysen.</p><section id="level5_32"><h4 id="h5_32">Normalitetsantakelsen</h4><p>N&#xe5;r det gjelder test for normalitet, handler dette om om feilleddet er normalfordelt. En mye brukt metode er Kolmogorow-Smirnov- og Shapiro-Wilkstestene. Disse testene beregner normalscoren til hvert estimat for feilleddet. Forskjellen mellom testene er at Kolmogorow-Smirnov benyttes hvis det er over 2 000 datapunkter, mens Shapiro-Wilks-testen benyttes n&#xe5;r det er under 2 000 datapunkter. Vi skal her forklare hvordan man tester normalitetsantakelsen til feilleddene. La oss f&#xf8;rst g&#xe5; tilbake utskriften til den hotellmodellen vi jobbet med, som hadde fem uavhengige variabler. Vi velger s&#xe5;:</p><figure id="imggroup_125" class="image"><img id="img_125" src="images/image125.jpg" alt="image" /><figcaption id="caption_122">Figur 10.11 Dialogboks for &#xe5; lagre residualer i dataarket.</figcaption></figure><p><em>R&#xf8;d trekant p&#xe5; response &gt; Save column &gt; Residuals</em></p><div id="page-251" class="page-normal" epub:type="pagebreak" title="251"></div><p>I datasettet vil du n&#xe5; se at det har kommet en ny kolonne helt til h&#xf8;yre i dataarket som heter Residual Omsetning.</p><p>Vi skal sjekke normaliteten til disse verdiene. Dette gj&#xf8;r vi ved &#xe5; velge:</p><p><em>Analyze</em> &gt; <em>Distribution</em> &gt; <em>Y, Columns</em></p><p>legg inn variabelen som er nederst, det vil si &#xab;<strong>Residual omsetning</strong>&#xbb;.</p><p>I utskriften velger man:</p><p><em>R&#xf8;d trekant p&#xe5; Residual omsetning</em> &gt; <em>Continious Fit</em> &gt; <em>Normal.</em></p><figure id="imggroup_126" class="image"><img id="img_126" src="images/image126.jpg" alt="image" /><figcaption id="caption_123">Figur 10.12 Dialogboks for &#xe5; lagre residualer i dataarket.</figcaption></figure><p>Utskriften vi velger vi deretter:</p><p><em>R&#xf8;d trekant ved Fitted normal</em> &gt; <em>Goodness-of-fit test.</em></p><div id="page-252" class="page-normal" epub:type="pagebreak" title="252"></div><figure id="imggroup_127" class="image"><img id="img_127" src="images/image127.jpg" alt="image" /><figcaption id="caption_124">Figur 10.13 Dialogboks for &#xe5; lagre residualer i dataarket.</figcaption></figure><p>Utskriften gir oss f&#xf8;lgende resultater:</p><figure id="imggroup_128" class="image"><img id="img_128" src="images/image128.jpg" alt="image" /><figcaption id="caption_125">Tabell 10.16 Utskrift fra test av normalitet p&#xe5; residualene i JMP.</figcaption></figure><p>Som vi ser fra utskriften, tilfredsstiller <em>ikke</em> residualene kravet om normalitet. Det betyr at nullhypotesen, som sier at feilleddet er normalfordelt, forkastes.</p><p>Legg merke til figuren &#xf8;verst. Den illustrerer respondenter som ligger utenfor resten av observasjonene, dette er med andre ord outliers. Vi kan enkelt teste <span id="page-253" class="page-normal" epub:type="pagebreak" title="253"></span>hva som skjer dersom vi ekskluderer disse casene. Dette gj&#xf8;r vi ved &#xe5; markere de med piltasten, h&#xf8;yreklikke og velge <em>Row exclude</em> og <em>Row hide.</em> Vi kj&#xf8;rer de samme analysene p&#xe5; nytt og f&#xe5;r f&#xf8;lgende resultater:</p><figure id="imggroup_129" class="image"><img id="img_129" src="images/image129.jpg" alt="image" /><figcaption id="caption_126">Tabell 10.17 Utskrift fra test av normalitet p&#xe5; residualene.</figcaption></figure><p>Som vi ser, var det disse tre observasjonene som skapte s&#xe5; mye st&#xf8;y i datasettet at normalitetskriteriet ikke ble oppfylt. Ved &#xe5; eliminere dem ser vi at vi kan beholde nullhypotesen, noe som betyr at residualene tilfredsstiller kriteriet til normalfordeling.</p></section></section><section id="level4_86"><h3 id="h4_86">Test av antakelse 4 &#x2013; konstant varians</h3><p>Avvik fra konstant varians kalles <em>heteroskedastisitet.</em> Dette kommer fra ordene <em>hetereo,</em> som betyr <em>forskjellig</em> p&#xe5; gresk, og <em>skedasis,</em> som betyr <em>fordeling.</em> Avvik fra denne antakelsen har vi dersom vi for eksempel har systematisk varierende <span id="page-254" class="page-normal" epub:type="pagebreak" title="254"></span>feilledd. Vi skal se p&#xe5; tilfellet hvor feilleddets varians varierer systematisk med en eller flere av forklaringsvariablene. Dette fenomenet kalles vanligvis for <em>hete-roskedastisitet,</em> til forskjell fra <em>homoskedastisitet</em> hvor feilleddet har konstant varians og er uavhengig av forklaringsvariablene. I tallmaterialet v&#xe5;rt ser vi p&#xe5; nederste figur i regresjonsutskriften som har plottet predicted mot residual.</p><figure id="imggroup_130" class="image"><img id="img_130" src="images/image130.jpg" alt="image" /><figcaption id="caption_127">Figur 10.14 Residualer plottet grafisk.</figcaption></figure><p>Ved &#xe5; studere figuren ser man om det er noe m&#xf8;nster som kan avdekke om variansen ikke er konstant. Dersom det ikke er noe systematisk m&#xf8;nster i plottet, er sannsynligvis ikke heteroskedastisitet noe problem. Ser man derimot et m&#xf8;nster hvor variansen tydelig &#xf8;ker (plottet ser ut som plog/vifte som brer seg utover) med &#xf8;kende grad, da har man muligens et heteroskedastisitetsproblem. I dette datasettet ser det ikke ut til &#xe5; v&#xe6;re noe m&#xf8;nster, og vi antar at dataene er homoskedastiske.</p><p>Dersom heteroskedastisitet er til stede, vil ikke <em>MKM</em>-estimatene lenger v&#xe6;re &#xab;effisiente&#xbb;. Dersom variansen til feilleddet ikke er konstant, men er en &#xf8;kende funksjon av <em>XI,</em> er det ikke usannsynlig, if&#xf8;lge formelen, for <em>var</em> <span class="asciimath">`(beta_1)(**) = beta_1`</span> at variansen til <span class="asciimath">`beta_1`</span> ogs&#xe5; &#xf8;ker siden telleren er estimatet for variansen til feilleddet.</p><p>N&#xe5; er det imidlertid slik at <em>MKM vekter</em> alle observasjoner likt. Som en f&#xf8;lge av dette vil <em>MKM</em> ikke ta <em>hensyn</em> til den forskjellige betydningen store og sm&#xe5; <em>X</em>-verdier m&#xe5;tte ha. Derfor kan <em>MKM</em> komme til &#xe5; estimere standardfeil som er for sm&#xe5;. Dette kan igjen f&#xf8;re til at <em>t</em>-verdiene blir for store, og vi kan komme til feilaktig &#xe5; forkaste nullhypotesen <span class="asciimath">`H_0: beta_i = 0`</span>.</p><p>Det finnes imidlertid estimeringsmetoder (<em>WLS</em> = Weighted Least Squares) som tar hensyn til dette og tilordner vekter til hver observert variabel som er invers proporsjonal til variansen til den aktuelle variabelen. Denne teknikken ligger imidlertid utenfor temaet til denne boka.</p></section><section id="level4_87"><div id="page-255" class="page-normal" epub:type="pagebreak" title="255"></div><h3 id="h4_87">Test av antakelse 6 &#x2013; Modellen skal v&#xe6;re line&#xe6;r i parametrene</h3><p>Vi antar at modellen skal v&#xe6;re line&#xe6;r i parametrene. Som for den enkle regresjons-modellen betyr dette at modellen skal v&#xe6;re line&#xe6;r i <span class="asciimath">`beta_o " og " beta_i (i = 1, 2, 3,..., k) " og " epsilon_i`</span>. Den kan imidlertid v&#xe6;re b&#xe5;de line&#xe6;r og ikke-line&#xe6;r i de uavhengige variablene. For eksempel er b&#xe5;de</p><p><span class="asciimath">`Y = beta_0 + beta_1 X_1 + beta_2 X_1^2 + epsilon " og "`</span></p><p><span class="asciimath">`Y = beta_0 + beta_1 X_1 + beta_2 X_2 + beta_3 X_1 X_2 + epsilon`</span></p><p>line&#xe6;re i parametrene selv om sammenhengen i de uavhengige variablene er ikke line&#xe6;r.</p><p>Ligningen <span class="asciimath">`Y = beta_0 + beta_1 X_1 + beta_2 X_1^(beta_3) + epsilon`</span> er derimot ikke line&#xe6;r i parameteren <span class="asciimath">`beta_3`</span> p&#xe5; grunn av at <span class="asciimath">`beta_3`</span> er eksponent i uttrykket <span class="asciimath">`X_1^(beta_3)`</span>.</p><p>Mer generelt kan linearitet i paremetrene uttrykkes i ligningen <span class="asciimath">`Y_i = beta_0 + beta_j * f(X_(ij)) + epsilon_i`</span>, hvor <em>j</em> 1,2,3,...<em>k. f</em> er en funksjon (line&#xe6;r eller ikkeline&#xe6;r) som <em>ikke</em> inneholder de parametre vi &#xf8;nsker &#xe5; estimere.</p></section><section id="level4_88"><h3 id="h4_88">Test av antakelse 7 &#x2013; kollinearitet (multikollinearitet)</h3><p>Kollinearitet handler om line&#xe6;r avhengighet mellom to (kollinearitet) eller flere (multikollinearitet) uavhengige variabler. En av forutsetningene for regresjonsanalyse (antakelse 7) krever at det <em>ikke</em> skal v&#xe6;re en eksakt line&#xe6;r avhengighet, <em>perfekt</em> kollinearitet, mellom forklaringsvariablene. Et slikt fenomen har vi for eksempel dersom vi konstruerer en multippel regresjonsmodell for &#xe5; forklare avlingsmengde (<em>Y</em>) som en funksjon av temperaturen m&#xe5;lt b&#xe5;de i Celsius-grader <span class="asciimath">`(X_1)`</span> og i Fahrenheit-grader <span class="asciimath">`(X_2).X_1 " og " X_2`</span> vil v&#xe6;re perfekt kolline&#xe6;re og dermed brytes antakelsen.</p><p>N&#xe5; vil perfekt kollinearitet sjelden by p&#xe5; problemer. Nedenfor vil vi vise at det rett og slett er umulig &#xe5; estimere regresjonsparametrene under slike forhold. N&#xe5;r vi i det f&#xf8;lgende behandler kollinearitet, vil det dreie seg om <em>ikke-perfekt kollinearitet.</em> Litt forenklet kan vi si at vi har et <em>problem</em> med ikke-perfekt kollinearitet n&#xe5;r relasjonen eller avhengigheten mellom de uavhengige variablene blir s&#xe5; sterk at det resulterer i un&#xf8;yaktige estimater for regresjonsparametrene. Derfor vil kollinearitet v&#xe6;re et &#xab;gradsfenomen&#xbb; som f&#xf8;rst gj&#xf8;r seg gjeldende n&#xe5;r relasjonen mellom de uavhengige variablene er sterk.</p><section id="level5_33"><div id="page-256" class="page-normal" epub:type="pagebreak" title="256"></div><h4 id="h5_33">M&#xe5;ling av kollinearitet</h4><p>Det finnes mange metoder for &#xe5; m&#xe5;le eller oppdage om kollinearitet er et problem. Vi skal konsentrere oss om to relativt enkle metoder: <em>Korrelasjonsmetoden</em> og <em>VIF-metoden.</em></p><p>Den f&#xf8;rste g&#xe5;r kort og godt ut p&#xe5; at man beregner korrelasjonskoeffisientene mellom alle forklaringsvariablene (de uavhengige variablene). Korrelasjoner i n&#xe6;rheten av -1 eller 1 kan tyde p&#xe5; at sammenhengen mellom variablene er sterk, og dermed kan vi f&#xe5; problemer med kollineariteten. N&#xe5; er det slik at det ikke alltid er like enkelt &#xe5; avgj&#xf8;re n&#xe5;r en korrelasjon er <em>n&#xe6;r</em> 1 (eller -1). En korrelasjon mellom to variabler som er over 0,9 (eller mindre enn -0,9), er n&#xe6;r 1 (-1), og da m&#xe5; man v&#xe6;re p&#xe5; vakt. Men hva med en korrelasjon lik f.eks. 0,78 (eller -0,78)? Er ikke dette ogs&#xe5; en indikasjon p&#xe5; en relativt sterk sammenheng? Jo, det er nok det, men det er ikke opplagt at det vil for&#xe5;rsake problematisk kollinearitet. Er derimot korrelasjonen et sted mellom -0,5 og 0,5, vil etter all sannsynlighet kollineariteten v&#xe6;re uproblematisk, men sikre kan vi ikke v&#xe6;re.</p><p>Tenk deg at du har estimert en regresjonsmodell med tre uavhengige variabler (tre <em>X</em>-variabler):</p><p><em>Analyze</em> &gt; <em>Multivariate methods</em> &gt; <em>Multivariate</em> &gt; <em>Y, column.</em></p><figure id="imggroup_131" class="image"><img id="img_131" src="images/image131.jpg" alt="image" /><figcaption id="caption_128">Figur 10.15 Dialogboks for korrelasjonsmatrise i JMP.</figcaption></figure><div id="page-257" class="page-normal" epub:type="pagebreak" title="257"></div><p>Dette gir f&#xf8;lgende korrelasjonsmatrise:</p><figure id="imggroup_132" class="image"><img id="img_132" src="images/image132.jpg" alt="image" /><figcaption id="caption_129">Figur 10.16 Korrelasjonsmatrise.</figcaption></figure><p>Korrelasjonsmatrisen forteller kun i hvilken grad to og to variabler samvarierer. Det kan for eksempel v&#xe6;re slik at salgsvekst er sterkt kolline&#xe6;r med en <em>line&#xe6;r kombinasjon</em> av l&#xf8;nnsvekst og antall ansatte. Dette vil vi ikke kunne oppdage ved bare &#xe5; studere korrelasjonene.</p><p>En metode som kan brukes i en slik situasjon, er VIF-testen. <em>VIF</em> er en forkortelse for Variance Inflation Factor og regnes ut av de fleste programvarer som kan behandle multippel regresjon. Dersom vi har en multippel regresjonsmodell med <em>k</em> stk. <em>X</em>-variabler, vil vi kunne beregne <em>k</em> stk. <em>VIF</em>-verdier, en for hver <em>X</em>-variabel. <em>VIF</em>-verdien til en <em>X</em>-variabel m&#xe5;ler i hvilken grad den aktuelle <em>X</em>-variabelen &#xab;lar seg forklare&#xbb; av de andre <em>X</em>-variablene i modellen. I JMP kj&#xf8;rer vi en vanlig regresjonsanalyse, for s&#xe5; &#xe5; gj&#xf8;re f&#xf8;lgende for &#xe5; f&#xe5; fram VIF-testen:</p><p><strong>Parameter Estimates</strong></p><table id="table_37"><tbody><tr><td rowspan="1" colspan="1"><strong>Term</strong></td><td rowspan="1" colspan="1"><strong>Estimate</strong></td><td rowspan="1" colspan="1"><strong>Std Error</strong></td><td rowspan="1" colspan="1"><strong>t Ratio</strong></td><td rowspan="1" colspan="1"><strong>Prob &gt;|t|</strong></td><td rowspan="1" colspan="1"><strong>VIF</strong></td></tr><tr><td rowspan="1" colspan="1">Intercept</td><td rowspan="1" colspan="1">0,3836364</td><td rowspan="1" colspan="1">2,265977</td><td rowspan="1" colspan="1">0,17</td><td rowspan="1" colspan="1">0,8653</td><td rowspan="1" colspan="1"></td></tr><tr><td rowspan="1" colspan="1">LONNSVEK</td><td rowspan="1" colspan="1">-0,129513</td><td rowspan="1" colspan="1">0,519836</td><td rowspan="1" colspan="1">-0,25</td><td rowspan="1" colspan="1">0,8036</td><td rowspan="1" colspan="1">1,3239112</td></tr><tr><td rowspan="1" colspan="1">SALGGVEK</td><td rowspan="1" colspan="1">0,2460976</td><td rowspan="1" colspan="1">0,575247</td><td rowspan="1" colspan="1">0,43</td><td rowspan="1" colspan="1">0,6695</td><td rowspan="1" colspan="1">1,313763</td></tr><tr><td rowspan="1" colspan="1">ANSATTE</td><td rowspan="1" colspan="1">0,6067037</td><td rowspan="1" colspan="1">0,022876</td><td rowspan="1" colspan="1">26,52</td><td rowspan="1" colspan="1"> &lt; ,0001*</td><td rowspan="1" colspan="1">1,0030699</td></tr></tbody></table><p>Tabell 10.18 Utskrift fra VIF-testen i JMP.</p><p>Man st&#xe5;r med musepila over tabellen i utskriften for <em>parameter estimater</em> og h&#xf8;yreklikker p&#xe5; musetasten. Der velger man</p><div id="page-258" class="page-normal" epub:type="pagebreak" title="258"></div><p><em>Columns</em> &gt; <em>VIF.</em></p><p>VIF regnes ut p&#xe5; denne m&#xe5;ten: <span class="asciimath">`VIF (x_j) = (1)/(1 - R_j^2)"j" = 1,2,3`</span></p><p>Merk at man her bruker <span class="asciimath">`X_j`</span> som avhengig variabel og de resterende <em>X</em>-variablene som uavhengige variabler for &#xe5; beregne <span class="asciimath">`R^2`</span>. Dersom <em>j</em> = 1, vil <span class="asciimath">`R^(2_1)`</span> v&#xe6;re determinasjonskoeffisienten for den estimerte regresjonsmodellen, og den tilh&#xf8;rende <em>VIF</em>-verdien regnes ut ved hjelp av formelen.</p><p>Enkelte mener at en <em>VIF</em>-verdi p&#xe5; 5,5 er den &#xf8;vre grensen for hvor sterk grad av kollinearitet vi kan tolerere. Andre vil hevde at kollinearitet f&#xf8;rst blir et problem n&#xe5;r <em>VIF-verdien</em> overstiger 10. Dette betyr at den tilh&#xf8;rende <span class="asciimath">`(R^(2_1)`</span> minst er lik 0,9. Som man skj&#xf8;nner, kan heller ikke <em>VIF</em>-verdiene gi oss noe endelig svar. Det som imidlertid er viktig, er at man er oppmerksom p&#xe5; at man kan ha et kollinearitetsproblem n&#xe5;r <em>VIF</em>-verdien overstiger 5. Sp&#xf8;rsm&#xe5;let er om det er <em>alvorlig nok.</em> I visse situasjoner kan regresjonsmodellen ha s&#xe5; mange andre gode kvaliteter at man er villig til &#xe5; akseptere at det er kollinearitet mellom forklaringsvariablene.</p></section><section id="level5_34"><h4 id="h5_34">Minste kvadraters metode</h4><p>Ideen bak minste kvadraters metode (MKM) er som vi tidligere har v&#xe6;rt inne p&#xe5;, &#xab;&#xe5; finne fram til parameterestimater&#xbb;</p><p><span class="asciimath">`hatbeta_0,hatbeta_1,hatbeta_2,...,hatbeta_k`</span></p><p>slik at summen</p><p><span class="asciimath">`sum(Y - hatY)^(2_4)`</span></p><p>blir s&#xe5; liten som overhodet mulig. Matematisk blir &#xab;summen av kvadrater derivert med hensyn p&#xe5; <span class="asciimath">`beta_0,beta_1,beta_2,...,beta_k&#xbb;`</span>. Deretter l&#xf8;ses et settmed ligninger av</p><p>typen <span class="asciimath">`del((sum(Y - hatY)^2))/(delbeta_i) = 0`</span></p><p>L&#xf8;sningene av disse ligningene gir oss formler som vi kan bruke til &#xe5; beregne parameter verdiene.</p><p><span class="asciimath">`Y = hatbeta_0 + hatbeta_1 X_1 + hatbeta_2 X_2 ...hatbeta_k X_k`</span>.</p><p>Regresjonsligningens forklaringskraft:</p><p><span class="asciimath">`Y_i = hatbeta_0 + hatbeta X_1 + e_i`</span> er ekvivalent med <span class="asciimath">`Y_i = barY_i + e_i`</span></p><div id="page-259" class="page-normal" epub:type="pagebreak" title="259"></div><p>I den siste ligningen trekker vi ifra <span class="asciimath">`barY`</span> p&#xe5; begge sider og f&#xe5;r</p><p><span class="asciimath">`Y_i = Y_i + e_i iff Y_i - barY = hatY - barY + e_i`</span></p><p>Dette gjelder for alle <em>i</em> (observasjoner).</p><p>Da dette gjelder for alle <em>i,</em> vil ligningen ogs&#xe5; gjelde om vi <em>summerer</em> over alle <em>i</em> p&#xe5; begge sider. Vi f&#xe5;r alts&#xe5;</p><p><span class="asciimath">`sum_(i=1)^n(Y_i = barY) = sum_(i=1)^n(hatY_i = barY) + sum_(i=1)^n e_i`</span></p><p>Det som ikke er like lett &#xe5; &#xab;se&#xbb;, er at da vil ogs&#xe5; f&#xf8;lgende ligning gjelde:</p><p><span class="asciimath">`sum_(i=1)^n(Y_i = barY)^2 = sum_(i=1)^n(hatY_i = barY)^2 + sum_(i=1)^n e_i^2`</span></p><p>TSS RSS ESS</p><p>I den siste ligningen har vi tre &#xab;summasjonsledd&#xbb;. Det f&#xf8;rste.</p><p><span class="asciimath">`sum_(i=1)^n(Y_i - barY)^2`</span></p><p>kaller vi total variasjon til <em>Y</em> med forkortelsen <em>TSS</em> (Total Sum of Squares). Det andre,</p><p><span class="asciimath">`sum_(i=1)^n(hatY_i - barY)^2`</span></p><p>kaller vi <em>variasjonen forklart av regresjonsmodellen</em> og benytter forkortelsen <em>RSS</em> (Regression Sum of Squares). Det siste, som er <em>summen av de kvadrerte residualer,</em></p><p><span class="asciimath">`sum_(i=1)^n e_i^2`</span></p><p>kalles <em>ESS</em>(Error Sum of Squares). Vi har da at</p><p><span class="asciimath">`TSS = RSS + ESS iff 1 = (RSS)/(TSS) + (ESS)/(TSS)`</span></p><p>Br&#xf8;ken <span class="asciimath">`(RSS)/(TSS)`</span>, som alts&#xe5; er lik</p><p><span class="asciimath">`(sum_(i=1)^n(hatY_i - barY)^2)/(sum_(i=1)^n(Y_i - barY)^2)`</span></p><p>kalles <em>andelen av forklart variasjon.</em></p><div id="page-260" class="page-normal" epub:type="pagebreak" title="260"></div><p><em>t</em>-testen:</p><p>Formler for utledning av <em>t</em>-testen:</p><p><span class="asciimath">`t = (beta)/(s_hatbeta) " hvor "`</span></p><p><span class="asciimath">`s_hatbeta = (s_e)/(sqrt(sum_(i=1)^n(X_i - barX)^2)) " og "`</span></p><p><span class="asciimath">`s_e = sqrt((1)/(n - 2)sum_(i=1)^n(Y_i - barY_i)^2) = sqrt((1)/(n - 2)sum_(i=1)^n e_i^2)`</span></p><p>Enkel regresjon og korrelasjon:</p><p>I kapittel 8 definerte vi korrelasjonskoeffisienten mellom to variabler <em>X</em> og <em>Y</em> ved</p><p><span class="asciimath">`" Corr " (X, Y) = (Cov(X, Y))/(s_xs_y) = (sum_(i=1)^n(X_i - barX) (Y_i - barY))/(sqrt(sum_(i=1)^n(X_i - barX)^2 sum_(i=1)^n(Y_i - barY)^2))`</span></p><p>P&#xe5; samme m&#xe5;te som <span class="asciimath">`hatbeta`</span> er estimat for <span class="asciimath">`beta`</span>, kan <em>Corr</em>(<em>X,Y</em>) ses som et estimat for en sann, men ukjent korrelasjonskoeffisient symbolisert ved <span class="asciimath">`rho`</span> . Akkurat som for <span class="asciimath">`beta`</span> kan vi teste en hypotese <span class="asciimath">`H_0: rho = 0`</span> mot et passende alternativ. Testobservatoren er gitt ved</p><p><span class="asciimath">`t = (" Corr "(X, Y))/(sqrt((1 - " Corr "(X, Y)^2)/(n - 2)`</span></p><p>og den f&#xf8;lger t-fordelingen med <em>n</em>-2 frihetsgrader.</p><p>Stigningskoeffisienten <span class="asciimath">`hatbeta`</span> og korrelasjonskoeffisienten <em>Corr</em>(<em>X,Y</em>) henger n&#xf8;ye sammen. Det er nemlig enkelt &#xe5; vise at f&#xf8;lgende ligning gjelder:</p><p><span class="asciimath">`hatbeta = (" Corr " (X,Y) * S_Y)/(S_X), " hvor " S_X " og " S_Y`</span></p><p>er standardavvikene for hhv. <em>X</em> og <em>Y</em>. Videre er det ogs&#xe5; slik at i enkel regresjonsanalyse er absoluttverdien av <em>Corr</em>(<em>X,Y</em>) lik kvadratroten av <span class="asciimath">`R^2`</span>. S&#xe5; hvis korrelasjonen mellom<em>X</em> og <em>Y</em> er 0,8, vil <span class="asciimath">`R^2`</span> i regresjonsligningen <span class="asciimath">`hatY = hatbeta_0 + hatbeta " X"`</span> v&#xe6;re lik 0,64 <span class="asciimath">`(R^2 0,8 * 0,8)`</span>.</p></section></section></section>
</body>
</html>