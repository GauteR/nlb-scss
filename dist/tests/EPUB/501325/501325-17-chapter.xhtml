<?xml version="1.0" encoding="utf-8"?><!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xml:lang="no" lang="no" epub:prefix="nordic: http://www.mtm.se/epub/">
<head xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:dcterms="http://purl.org/dc/terms/">
    <meta charset="UTF-8" />
    <title>Metode og dataanalyse</title>
    <meta name="dc:identifier" content="501325" />
    <meta name="viewport" content="width=device-width" />
    <meta name="nordic:guidelines" content="2015-1" />
    <meta name="nordic:supplier" content="Planman" />
    <meta name="nordic:supplieddate" content="2011-10-04" />
    <meta name="nordic:signum" content="" />
    <meta content="Gripsrud, Geir" name="dc:creator" />
    <meta content="Olsson, Ulf Henning" name="dc:creator" />
    <meta content="Silkoset, Ragnhild" name="dc:creator" />
    <meta content="2011-10-04" name="dc:date" />
    <meta content="NLB" name="dc:publisher" />
    <meta content="no" name="dc:language" />
    <meta content="urn:isbn:978-82-7634-864-4-1" name="dc:source" />
    <style type="text/css" xml:space="preserve">
                .initialism{
                    -epub-speak-as:spell-out;
                }
                .list-style-type-none{
                    list-style-type:none;
                }
                table[class ^= "table-rules-"],
                table[class *= " table-rules-"]{
                    border-width:thin;
                    border-style:hidden;
                }
                table[class ^= "table-rules-"]:not(.table-rules-none),
                table[class *= " table-rules-"]:not(.table-rules-none){
                    border-collapse:collapse;
                }
                table[class ^= "table-rules-"] td,
                table[class *= " table-rules-"] td{
                    border-width:thin;
                    border-style:none;
                }
                table[class ^= "table-rules-"] th,
                table[class *= " table-rules-"] th{
                    border-width:thin;
                    border-style:none;
                }
                table.table-rules-none td,
                table.table-rules-none th{
                    border-width:thin;
                    border-style:hidden;
                }
                table.table-rules-all td,
                table.table-rules-all th{
                    border-width:thin;
                    border-style:solid;
                }
                table.table-rules-cols td,
                table.table-rules-cols th{
                    border-left-width:thin;
                    border-right-width:thin;
                    border-left-style:solid;
                    border-right-style:solid;
                }
                table.table-rules-rows tr{
                    border-top-width:thin;
                    border-bottom-width:thin;
                    border-top-style:solid;
                    border-bottom-style:solid;
                }
                table.table-rules-groups colgroup{
                    border-left-width:thin;
                    border-right-width:thin;
                    border-left-style:solid;
                    border-right-style:solid;
                }
                table.table-rules-groups tfoot,
                table.table-rules-groups thead,
                table.table-rules-groups tbody{
                    border-top-width:thin;
                    border-bottom-width:thin;
                    border-top-style:solid;
                    border-bottom-style:solid;
                }
                table[class ^= "table-frame-"],
                table[class *= " table-frame-"]{
                    border:thin hidden;
                }
                table.table-frame-void{
                    border-style:hidden;
                }
                table.table-frame-above{
                    border-style:outset hidden hidden hidden;
                }
                table.table-frame-below{
                    border-style:hidden hidden outset hidden;
                }
                table.table-frame-lhs{
                    border-style:hidden hidden hidden outset;
                }
                table.table-frame-rhs{
                    border-style:hidden outset hidden hidden;
                }
                table.table-frame-hsides{
                    border-style:outset hidden;
                }
                table.table-frame-vsides{
                    border-style:hidden outset;
                }
                table.table-frame-box{
                    border-style:outset;
                }
                table.table-frame-border{
                    border-style:outset;
                }</style>
    <link rel="stylesheet" type="text/css" href="css/accessibility.css" />
    <link rel="prev" href="501325-16-chapter.xhtml" />
    <link rel="next" href="501325-18-chapter.xhtml" />
</head>
<body id="level2_11" epub:type="bodymatter chapter">
<div id="page-261" class="page-normal" epub:type="pagebreak" title="261"></div><h1 id="h2_11">Kapittel 11 Segmentering og clustering</h1><section id="level3_69"><h2 id="h3_69">11.1 Innledning</h2><p>I dette kapitlet skal vi behandle ulike metoder for segmentering av data, med andre ord metoder som brukes for &#xe5; splitte data opp i mindre grupper, s&#xe5;kalte <em>clustre,</em> som hver kan gis en forretningsforst&#xe5;else, og som kan behandles separat videre. Et cluster er rett og slett en naturlig gruppering av data, gjort p&#xe5; en slik m&#xe5;te at objekter som ligger i samme cluster, ligner p&#xe5; hverandre i stor grad, mens de ligner mindre p&#xe5; objekter som finnes i andre grupper. Vi &#xf8;nsker med andre ord &#xe5; finne grupper som er s&#xe5; sammenhengende og homogene som mulig.</p><p>Mennesker er intuitivt flinke til &#xe5; gj&#xf8;re slike oppgaver. Allerede fra tidlig barndom er vi i stand til &#xe5; se forskjell p&#xe5; for eksempel stokkender og kr&#xe5;ker, til tross for de mange egenskapene som artene tross alt har til felles, slik som at de har nebb, fj&#xe6;r, de flyr, etc. Clustering handler p&#xe5; samme m&#xe5;te om &#xe5; finne fram til de <em>viktigste, mest definerende</em> egenskapene som skiller gruppene og som gj&#xf8;r at vi kan kategorisere objekter p&#xe5; en meningsfylt m&#xe5;te. Det er med andre ord en viktig distinksjon mellom clustering og tradisjonelle segmenteringsmetoder &#x2013; innen clustering forutsetter vi ikke at spesifikke variabler skal brukes, snarere lar data selv vise oss hvilke naturlige grupperinger som finnes.</p><p>Innenfor markedsf&#xf8;ring vil clusteringmetoder kunne benyttes som en metodikk for <em>datautforskning,</em> for &#xe5; forst&#xe5; kundene bedre. For eksempel kan vi anvende slike metoder p&#xe5; data fra sp&#xf8;rreskjema, hvor personers svar vil ligne p&#xe5; hverandre i st&#xf8;rre eller mindre grad. Personer som har svart likt p&#xe5; mange av sp&#xf8;rsm&#xe5;lene, vil naturlig nok anses som &#xab;n&#xe6;rmere hverandre&#xbb; enn personer som har svart helt annerledes i unders&#xf8;kelsen. Hvilke naturlige grupper kan vi s&#xe5; dele kundene inn i og hvilke egenskaper er framtredende for de ulike gruppene &#x2013; hvem er de, hva er de opptatt av og hvordan kan vi f&#xe5; best mulig kontakt med dem?</p><p>I det f&#xf8;lgende skal vi se p&#xe5; data over <em>k</em>-variabler, hvert datapunkt (som representerer f. eks en person) vil da v&#xe6;re representert som en vektor med <em>k</em>-dimensjoner, og igjen et punkt i det <em>k</em>-dimensjonale rom. I denne modellen er det intuitivt &#xe5; tenke seg avstanden mellom to punkter som et uttrykk for likhet mellom punktene. Vi kan videre tenke oss at vi kan finne grupper som &#xab;skyer&#xbb; i dette rommet, hvor avstandene innad i skyen er mindre enn avstandene mellom skyene. Dette <span id="page-262" class="page-normal" epub:type="pagebreak" title="262"></span>er det enkle prinsippet som ligger bak mange av clusteringmetodene. I det f&#xf8;lgende vil vi ta for oss hvordan vi kan formalisere likhetsbegrepet som et uttrykk for avstand, vi skal snakke om to enkle, men ofte brukte modeller og avslutningsvis hvordan vi automatisk kan gruppere <em>p&#xe5; variabelniv&#xe5; &#x2013;</em> s&#xe5;kalt variabel clustering.</p></section><section id="level3_70"><h2 id="h3_70">11.2 Likhet mellom grupper i dataene</h2><p>Som vi nevnte i innledningen, &#xf8;nsker vi &#xe5; finne fram til de viktigste egenskapene for &#xe5; definere rimelige grupper i dataene v&#xe5;re. Med andre ord, hvilke variabler er det som er best egnet til &#xe5; definere klart atskilte grupper? Denne framgangsm&#xe5;ten har sine utfordringer, se p&#xe5; f&#xf8;lgende enkle eksempel;</p><p>Hva ligner mest p&#xe5; en and? En kr&#xe5;ke eller en pingvin?</p><p>Dersom vi ser p&#xe5; egenskapen &#xab;&#xe5; fly&#xbb;, ligner kr&#xe5;ka mest, dersom vi derimot ser p&#xe5; egenskapen &#xab;&#xe5; sv&#xf8;mme&#xbb;, ligner pingvinen mest.</p><p>Hvordan kan vi s&#xe5; formalisere begrepet likhet? Den enkleste og mest fundamentale m&#xe5;ten er den s&#xe5;kalte euklidske distanse. I to dimensjoner kan vi se p&#xe5; denne rett og slett som avstanden mellom to punkter p&#xe5; en flate, hvor avstanden (hypotenusen) er definert som kvadratroten av summen av de kvadrerte lengdene av de to sidene i trekanten. Dersom vi generaliserer denne tanken, vil den euklidske distansen mellom to punkter <em>i</em> og <em>j</em> (i det <em>k</em>-dimensjonale rom, som representerer <em>k</em>-variabler) kunne beregnes via formelen</p><p><span class="asciimath">`" Dist"(i, j)= sqrt((x_(i1)-x_(j1))^2 +(x_(i2)-x_(j2))^2 + cdots + (x_(ik)-x_(jk))^2) = sqrt(sum_k (x_i - x_j)^2`</span></p><p>Denne formelen fungerer best n&#xe5;r den anvendes p&#xe5; numeriske, kontinuerlige variabler. Dersom man skal gruppere firmaer f.eks., vil eksempler p&#xe5; slike variabler v&#xe6;re omsetning, skatteniv&#xe5;, aksjekurs etc. Et viktig poeng er dog at algoritmene er sensitive for enheter &#x2013; det betyr at variabler med store tall (f.eks. omsetning) vil ha st&#xf8;rre innflytelse p&#xe5; grupperingen enn variabler med lave verdier (f.eks. prosentvis avkastning). Videre vil endring av m&#xe5;leenhet p&#xe5; en eller flere variabler (f.eks. endring av valuta p&#xe5; omsetning fra kroner til dollar) p&#xe5;virke resultatene kraftig. L&#xf8;sningen p&#xe5; dette problemet er &#xe5; <em>standardisere</em> variablene. Dette betyr i praksis at alle variablene transformeres ned p&#xe5; en felles, enhetsl&#xf8;s skala slik at tallene blir sammenlignbare samtidig som de relative st&#xf8;rrelsene blir bevart. Dette kan for eksempel gj&#xf8;res via f&#xf8;lgende formler &#x2013; vi har <em>n</em> m&#xe5;linger for variabel <em>v</em> &#x2013; verdien <span class="asciimath">`x_(iv)`</span> representerer verdi nummer <em>i</em> av variabel <em>v</em>:</p><div id="page-263" class="page-normal" epub:type="pagebreak" title="263"></div><p><span class="asciimath">`s_v = (1)/(n) (|x_(iv) - m_v| + |x_(2v) - m_v|+ cdots +|x_(kv) - m_v|)`</span></p><p><span class="asciimath">`z_(iv) = ((x_(iv) - m_v))/(s_v), i = 1, ...,n`</span></p><p>I formelen over representerer <span class="asciimath">`m_v`</span> gjennomsnittsverdien for variabel v over de <em>n</em> observasjonene. Uttrykket <span class="asciimath">`s_v`</span> i sin tur representerer rett og slett gjennomsnittlig avvik fra gjennomsnittsverdien, m.a.o. et uttrykk for variasjonen, m&#xe5;lt i variabelens opprinnelige skala. <span class="asciimath">`z_(iv)`</span> viser n&#xe5;, for hvert av de <em>n</em> objektene i datasettet hvor mange slike steg en gitt verdi ligger fjernet fra middelverdien for variabelen. Dette gir oss mulighet til, for enhver kontinuerlig variabel, &#xe5; plotte hvert punkt p&#xe5; <em>z</em>-skalaen &#x2013; hvor mange gjennomsnittsawik <span class="asciimath">`s_v`</span> et punkt ligger bortefra gjennomsnittet. Denne variabelen kan v&#xe6;re positiv eller negativ, men har en forventningsverdi p&#xe5; 0. For eksempel, dersom gjennomsnittlig alder <span class="asciimath">`m_(alder)`</span> i et datasett er 40 &#xe5;r og <span class="asciimath">`s_(alder)`</span> er 15 &#xe5;r, vil en 55-&#xe5;ring f&#xe5; z-verdi 1, en 70-&#xe5;ring 2 og en 25-&#xe5;ring verdien -1. P&#xe5; denne m&#xe5;ten kan vi s&#xe5; representere alle (numeriske) variabler som sammenlignbare verdier p&#xe5; samme skala.</p><p>Det finnes ogs&#xe5; spesialtransformasjoner som gj&#xf8;res for andre typer variabler, for eksempel bin&#xe6;re eller kategoriske variabler. Kategoriske typer vil kunne modelleres ved &#xe5; lage nye bin&#xe6;re dummy-variabler (p&#xe5; samme m&#xe5;ten som det gj&#xf8;res innenfor regresjon). For bin&#xe6;re variabler &#xf8;nsker vi i utgangspunktet &#xe5; skille variabler hvor de to mulige utfallene har like stor vekt og informasjonsverdi &#x2013; <em>kj&#xf8;nn</em> (f.eks. 50 % menn, 50 % kvinner) kan v&#xe6;re en slik variabel. Andre variabler kan v&#xe6;re vesentlig mer spesifikke og informasjonsb&#xe6;rende &#x2013; et eksempel p&#xe5; en slik kan v&#xe6;re hvorvidt vedkommende har en gitt sykdom (f.eks. 0,01 % har verdi 1 = ja/sann, 99,99 % verdien 0 = nei/usann). Det er &#xe5;penbart at den siste variabelen gir mer spiss informasjon som ev. vil v&#xe6;re til stor hjelp i tolkningen av clustrene (dette betyr ikke at kj&#xf8;nn ikke vil eller kan v&#xe6;re signifikant for gruppeinndelingen). For slike variabler kan man for eksempel velge &#xe5; ignorere informasjonen som ligger i <em>frav&#xe6;ret</em> av egenskapen som variabelen uttrykker, og kun ta hensyn til tilfeller hvor verdien er l/sann.</p></section><section id="level3_71"><div id="page-264" class="page-normal" epub:type="pagebreak" title="264"></div><h2 id="h3_71">11.3 Ulike metodikker innenfor clustering</h2><p>M&#xe5;let for clusteringprosessen er alts&#xe5; &#xe5; s&#xf8;rge for at &#xab;skyene&#xbb; vi finner, blir tettest mulig og samtidig godt separert fra hverandre i det <em>k</em>-dimensjonale rommet slik det er definert av variablene. Vi skiller mellom ikke-hierarkiske og hierarkiske metoder, hvor den f&#xf8;rste metoden resulterer i en splitt av datasettet i et antall grupper som spesifisert av brukeren, mens den hierarkiske metoden bygger opp et hierarki av clustre.</p></section><section id="level3_72"><h2 id="h3_72">11.4 Ikke-hierarkisk clustering</h2><p>Et klassisk eksempel p&#xe5; ikke-hierarkiske clusteringmetoder er den s&#xe5;kalte K-Means Clustering. Algoritmen benytter par ameteren <em>K</em> somrepresenterer antall clustre som skal beregnes, denne m&#xe5; alts&#xe5; ikke forveksles med <em>k</em> som er antall variabler i datasettet v&#xe5;rt. Input til metoden er et datasett med et antall (numeriske) variabler, resultatet er et sett med grupper eller clustre, representert ved sine sentre i data, ogs&#xe5; kalt centroider. Som tidligere nevnt antar vi at vi kan definere grad av likhet som avstand mellom punkter i det <em>k</em>-dimensjonale rom. Framgangsm&#xe5;ten er som f&#xf8;lger:</p><ul id="list_100"><li id="li_409">Anta eller estimer K &#x2013; antall clustre.</li><li id="li_410">Velg K tilfeldige punkter i det k-dimensjonale rom = (forel&#xf8;pige) centroider i clustrene.</li><li id="li_411">Repeter.</li><li id="li_412">Tilordne hvert punkt til det clusteret hvis sentrum ligger n&#xe6;rmest punktet.</li><li id="li_413">Flytt hver centroide slik at den n&#xe5; ligger i sentrum av sitt cluster.</li><li id="li_414">Inntil stabilitet.</li></ul><p>Algoritmen er ferdig (konvergert) n&#xe5;r de centroidepunktene ikke lenger flytter seg og clustrene dermed er stabile. Vi kan n&#xe5; hente ut statistikker for de ulike clustrene, for eksempel vil clusterets gjennomsnitt for <em>k</em> ulike variabler rett og slett v&#xe6;re centroidens posisjon i det <em>k</em>-dimensjonale rom. Videre vil algoritmen typisk rapportere hvor samlet/veldefinert clusteret er, med andre ord hvor tett medlemmene ligger samlet rundt centroiden, samt avstand til centroidene i de andre clustrene.</p><p>Som beskrevet forutsetter algoritmen at <em>K &#x2013;</em> antall clustre bestemmes. En del analyseverkt&#xf8;y vil kunne sette en verdi for <em>K</em> for deg, mens mange overlater denne beslutningen til brukeren. I det siste tilfellet anbefales det &#xe5; teste ut ulike verdier for parameteren &#x2013; dette for &#xe5; unng&#xe5; &#xe5; p&#xe5;tvinge en spesifikk gruppering p&#xe5; datasettet.</p><div id="page-265" class="page-normal" epub:type="pagebreak" title="265"></div><p>La oss studere f&#xf8;lgende eksempel: En fiktiv varekjede i en tenkt by har hentet ut et utvalg av noen av sine mest trofaste kunder. Datasettet inneholder data om 150 personer, og f&#xf8;lgende variabler:</p><ul id="list_101"><li id="li_415">Kj&#xf8;nn</li><li id="li_416">Bydel</li><li id="li_417">Alder</li><li id="li_418">Variabler som beskriver kundens kj&#xf8;p innen fire varegrupper:</li><li id="li_419">Kl&#xe6;r: kjop_avdl_klaer</li><li id="li_420">Elektronikk: kjop_avd2_elektro</li><li id="li_421">Bilutstyr:kjop_avd3_bil</li><li id="li_422">Hus og hjem:kjop_avd4_hjem</li><li id="li_423">En variabel som beskriver kundens kj&#xf8;p totalt</li><li id="li_424">kjop_total</li></ul><p>Vi &#xf8;nsker &#xe5; fors&#xf8;ke ut clustering for &#xe5; hente ut mer informasjon om disse kundene, og finne hvilke naturlige grupper som eventuelt utkrystalliserer seg. Det er ikke i utgangspunktet sikkert at clustering vil gi gode og meningsfylte resultater, men det kan allikevel anbefales &#xe5; gjennomf&#xf8;re clustering, ikke minst p&#xe5; grunn av den forst&#xe5;elsen av data som er en f&#xf8;lge av selve &#xf8;velsen.</p><p>Vi velger &#xe5; clustre p&#xe5; de fire varegruppe-variablene og &#xe5; generere fire clustre (valget av antall clustre, <em>K,</em> er som tidligere nevnt ikke trivielt, og det anbefales &#xe5; fors&#xf8;ke ulike verdier). I JMP velger vi</p><p><em>Analyse</em> &gt; <em>Multivariate</em> &gt; <em>Cluster analysis</em></p><p>de fire variablene, clustermetode K-Means og 4 clustre. Skjermbildet vil n&#xe5; se slik ut, og vi velger &#xab;OK&#xbb;:</p><figure id="imggroup_133" class="image"><img id="img_133" src="images/image133.jpg" alt="image" /><figcaption id="caption_130">Figur 11.1 Clustering analyse i JMP.</figcaption></figure><div id="page-266" class="page-normal" epub:type="pagebreak" title="266"></div><p>Vi kan n&#xe5; f&#xf8;lge iterasjonen stegvis slik algoritmen beskriver, eller velge &#xab;GO&#xbb; som kj&#xf8;rer det hele og gir oss de fire endelige gruppene.</p><figure id="imggroup_134" class="image"><img id="img_134" src="images/image134.jpg" alt="image" /><figcaption id="caption_131">Figur 11.2 Dialogboks for clustering analyse.</figcaption></figure><p>Vi velger n&#xe5; &#xe5; lagre resultatene &#x2013; &#xab;Save clusters&#xbb;, hvilket utvider datasettet med cluster-informasjon for hvert av de 150 individene. For &#xe5; f&#xe5; frem resultatet grafisk velger vi:</p><p><em>R&#xf8;d trekant</em> &gt; <em>Biplot</em></p><figure id="imggroup_135" class="image"><img id="img_135" src="images/image135.jpg" alt="image" /><figcaption id="caption_132">Figur 11.3 Utskrift fra clustering analyse i JMP.</figcaption></figure><div id="page-267" class="page-normal" epub:type="pagebreak" title="267"></div><p>Figur 11.3 viser at vi har oppn&#xe5;dd fire rimelig separerte clustre, de to dimensjonene (Prinl og Prin2) viser prinsipalkomponentene, og et s&#xe5;kalt loadings-plot viser korrelasjonen mellom de to prinsipalkomponentene og de fire opprinnelige variablene. Det er umiddelbart i&#xf8;ynefallende at oppf&#xf8;rselen til variabelen kjop_avd2_elektro skiller seg sterkt fra de tre andre, som seg imellom synes rimelig korrelerte. P&#xe5; samme m&#xe5;ten ser vi at cluster 4 skiller seg fra de andre tre, og et vesentlig bidrag her er nettopp denne variabelen.</p><p>Over figuren vises ogs&#xe5; p&#xe5; tabellform hvor gjennomsnittspunktene (centro-idene) ligger i de fire dimensjonene. Legg for eksempel merke til hvordan variabelen for hus og hjem er distribuert lengst til h&#xf8;yre i tabellen &#x2013; hvilket kan gi oss viktig informasjon i tolkningen av gruppene.</p></section><section id="level3_73"><h2 id="h3_73">11.5 Fortolkning og videre bruk av clustre</h2><p>Etter at clusteringen er gjennomf&#xf8;rt sitter vi med fire distinkte grupper og &#xf8;nsker &#xe5; gi disse en fortolkning. Vi anbefaler at de ulike gruppene vurderes og dokumenteres, slik at resultatene kan gi en forretningsforst&#xe5;else, og vi skal gi et par eksempler p&#xe5; slike i det f&#xf8;lgende. I dokumentasjonsprosessen anvender vi b&#xe5;de data som inngikk i clusteringen, i tillegg til andre variabler som m&#xe5;tte v&#xe6;re tilgjengelige, hvilket gir operasjonell kunnskap (hvordan bruker vi denne informasjonen) , men ogs&#xe5; verdifulle innspill til en kontinuerlig prosess mot forbedring av analysene og ikke minst den underliggende datakvaliteten. La oss f&#xf8;rst kikke p&#xe5; noen enkle histogrammer over grupperingsvariablene mot clustertilh&#xf8;righet.</p><figure id="imggroup_136" class="image"><img id="img_136" src="images/image136.jpg" alt="image" /><figcaption id="caption_133">Figur 11.4 Gruppe 4 i clustering analysen.</figcaption></figure><p>Framstillingen over viser sammensetning av kj&#xf8;psm&#xf8;nstrene for cluster 4 som vi har snakket om tidligere. Vi ser at de kj&#xf8;per relativt lite/er lite opptatt av kl&#xe6;r og hjem, mens bilutstyr og spesielt elektronikk er popul&#xe6;re varer hos disse kundene. Vi kan kalle denne gruppen &#xab;Teknologene&#xbb;. Den neste gruppen, 3, ser vi nedenfor har en helt annen fordeling av kj&#xf8;p. Disse kundene er opptatt av kl&#xe6;r, de kj&#xf8;per for store summer utstyr b&#xe5;de til bil og hjem, men ligger tydeligvis <span id="page-268" class="page-normal" epub:type="pagebreak" title="268"></span>p&#xe5; gjennomsnittet hva gjelder elektronikk. Et naturlig oppf&#xf8;lgingssp&#xf8;rsm&#xe5;l vil kunne v&#xe6;re hvorfor elektronikk er s&#xe5; lite etterspurt i denne kundegruppen, og ikke minst hvordan vi kan bruke denne informasjonen. La oss kalle denne gruppen &#xab;Statusjegerne&#xbb;.</p><figure id="imggroup_137" class="image"><img id="img_137" src="images/image137.jpg" alt="image" /><figcaption id="caption_134">Figur 11.5 Gruppe 3 i clustering analysen.</figcaption></figure><p>Vi kan l&#xe6;re mer om de ulike gruppene ved &#xe5; sammenstille dem med de &#xf8;vrige variablene som m&#xe5;tte v&#xe6;re tilgjengelige som &#xf8;konomiske og demografiske data, interesser, forbruksm&#xf8;nstre eller annen personinformasjon. Vi kan for eksempel sammenstille de to gruppene som vi har diskutert, med de andre variablene, i dette tilfellet kj&#xf8;nn, bydel, total kj&#xf8;pesum siste &#xe5;r og alder.</p><p>Vi ser at Cluster 4 &#x2013; &#xab;Teknologene&#xbb; &#x2013; er relativt unge mennesker, som lever spredt utover byen (bortsett fra Vest), og de har et relativt lavt totalkj&#xf8;p, gruppen synes &#xe5; v&#xe6;re jevnt fordelt blant begge kj&#xf8;nn.</p><figure id="imggroup_138" class="image"><img id="img_138" src="images/image138.jpg" alt="image" /><figcaption id="caption_135">Figur 11.6 Gruppe 4 i clustering analysen.</figcaption></figure><p>Umiddelbart kommer det ikke som noen overraskelse at cluster 3 ligger p&#xe5; topp hva gjelder bidrag, vi ser at sv&#xe6;rt mange av dem bor i Vest og at de aldersmessig ligger ganske spredt, heller ikke her synes det &#xe5; v&#xe6;re vesentlige kj&#xf8;nnsmessige forskjeller.</p><div id="page-269" class="page-normal" epub:type="pagebreak" title="269"></div><figure id="imggroup_139" class="image"><img id="img_139" src="images/image139.jpg" alt="image" /><figcaption id="caption_136">Figur 11.7 Gruppe 3 i clustering analysen.</figcaption></figure><p>I de to siste gruppene, 1 og 2, fant vi ogs&#xe5; noen interessante m&#xf8;nstre &#x2013; gruppe 1 best&#xe5;r av relativt unge mennesker som bruker mest p&#xe5; hjem og en del p&#xe5; kl&#xe6;r &#x2013; vi kan kalle dem &#xab;De Nyetablerte&#xbb;. Gruppe 2 i sin tur bruker ogs&#xe5; relativt mye p&#xe5; hus og kl&#xe6;r, men de ligger generelt h&#xf8;yere over alle fire produktgrupper og rangerer derfor naturligvis h&#xf8;yere i totalt kj&#xf8;p. Et passende navn for denne gruppen kan v&#xe6;re &#xab;De Lojale&#xbb;.</p><p>I den neste grafen ser vi p&#xe5; variablene cluster og bydel mot elektronikkj&#xf8;p. P&#xe5; h&#xf8;yre side i grafen er det vanskelig &#xe5; se umiddelbare sammenhenger mellom geografi og handling av denne typen varer. Dette kan vi bekrefte ved &#xe5; foreta en t-test som avsl&#xf8;rer signifikans i parvis sammenligninger av gruppene (se kapittel 9). For de fem bydelene kan vi umiddelbart konkludere at det ikke finnes vesentlige forskjeller for elektronikkj&#xf8;p mellom alle fire clustre (dette vises grafisk via de fem overlappende grupperingene p&#xe5; h&#xf8;yre side, og verifiseres av resultat-tabellene).</p><p>N&#xe5;r det gjelder elektronikkj&#xf8;p for de fire clustrene, kan vi ved &#xe5; studere gruppene fra t-testen umiddelbart konkludere at det finnes vesentlige forskjeller. Alle fire er signifikant forskjellige, de clustrene som ligner hverandre mest, er 2 og 3 (&#xab;De Lojale&#xbb; og &#xab;Statusjegerne&#xbb;), vi ser dette ved at disse ringene er n&#xe6;rmest hverandre i figuren.</p><figure id="imggroup_140" class="image"><img id="img_140" src="images/image140.jpg" alt="image" /><figcaption id="caption_137">Figur 11.8 t-test av forskjeller mellom grupper fra clustering analyse.</figcaption></figure><div id="page-270" class="page-normal" epub:type="pagebreak" title="270"></div><p>Figuren nedenfor viser hvordan man kan <em>sammenstille</em> informasjon fra flere variabler i ett diagram, og det er interessant &#xe5; se at det finnes mulige <em>samspill</em> &#x2013; avhengigheter mellom variablene. Vi viser her et s&#xe5;kalt trekart</p><p><em>Graph</em> &gt; <em>Tree Map</em></p><p>vi har her sammenstilt de tre variablene som var nevnt ovenfor &#x2013; ved &#xe5; kombinere cluster-inndeling med bydel og f&#xe5;tt et rutekart som inneholder 25 ruter. <em>St&#xf8;rrelsen</em> p&#xe5; hver rute representerer elektronikkj&#xf8;p for gruppen, med andre ord summen av enkeltindividenes kj&#xf8;p (kjop_avd2_elektro). <em>Fargen</em> p&#xe5; hver rute i mosaikken viser gruppens <em>gjennomsnittlige</em> totalkj&#xf8;p av elektronikk &#x2013; med andre ord store lyse ruter representerer mange individer som hver har et lite bidrag, mens sm&#xe5; m&#xf8;rke ruter inneholder et f&#xe5;tall personer, men hvor hver i gjennomsnitt kj&#xf8;pte for et stort bel&#xf8;p. Vi ser ogs&#xe5; her at cluster 4 dominerer mht. kj&#xf8;p, mens gruppe 1 og 2 synes temmelig uinteresserte i elektronikk. Gruppe 1, 2 og 3 synes alle &#xe5; v&#xe6;re temmelig jevne p&#xe5; tvers av geografi. Imidlertid kan vi hos cluster 4 &#x2013; &#xab;Teknologene&#xbb; nederst til h&#xf8;yre ogs&#xe5; se visse geografiske forskjeller, for eksempel at bydel Nord utmerker seg i positiv forstand, selv i dette teknologiorienterte segmentet. Denne observasjonen kan gi anspor til videre analyser for &#xe5; verifisere og finne ut hvordan eventuelt &#xe5; utnytte denne informasjonen &#x2013; finnes det for eksempel forskjeller i hvordan elektroniske produkter blir promotert i nord?</p><figure id="imggroup_141" class="image"><img id="img_141" src="images/image141.jpg" alt="image" /><figcaption id="caption_138">Figur 11.9 Dialogboks for trekart i JMP.</figcaption></figure><figure id="imggroup_142" class="image"><img id="img_142" src="images/image142.jpg" alt="image" /><figcaption id="caption_139">Figur 11.10 Gruppering av clusters.</figcaption></figure><div id="page-271" class="page-normal" epub:type="pagebreak" title="271"></div><p>Clustering er som nevnt i hovedsak en metode for &#xe5; utforske, bli kjent med data. Imidlertid er det gode grunner til &#xe5; vurdere &#xe5; ta denne informasjonen inn i den analytiske prosessen. For eksempel kan det v&#xe6;re en mulighet &#xe5; dele opp data i henhold til clustrene og behandle hvert av de nye datasettene spesielt, for eksempel for &#xe5; utvikle prediktive modeller. Antakelsen er da at clustrene representerer grupper som er s&#xe5; unike og ulike at de med fordel b&#xf8;r modelleres videre hver for seg. En annen mulighet er &#xe5; gj&#xf8;re flere clustering-operasjoner p&#xe5; de samme data ved &#xe5; splitte opp variablene i undergrupper. Slik kan man for eksempel lage &#xab;Oppf&#xf8;rselsclustre&#xbb; som kan sammenstilles for eksempel med &#xab;Demografiske clustre&#xbb;, hvilket kan v&#xe6;re en interessant &#xf8;velse i seg selv, samt gi vesentlige innspill til videre analyse.</p></section><section id="level3_74"><h2 id="h3_74">11.6 Hierarkisk clustering</h2><p>Hierarkisk clustering bygger, p&#xe5; basis av et datasett, en trestruktur som har en helt spesifikk tolkning. Metodene kan grovt deles inn i to grupper, nemlig delende (divisive) og samlende (agglomerative) &#x2013; for den sistes del innleder metoden med &#xe5; definere et nytt cluster p&#xe5; basis av de to punktene som ligger n&#xe6;rmest hverandre. Dernest startes en iterativ prosess hvor sammensl&#xe5;ing skjer mellom de clustrene/punktene som ligger n&#xe6;rmest hverandre. I steg to kan med andre ord en av to ting skje: Vi kan finne at gruppen som vi nettopp lagde (som inneholder to punkter), ligger n&#xe6;rt et tredje punkt &#x2013; i dette tilfelle dannes et nytt cluster som n&#xe5; inneholder tre punkter. Den andre muligheten er at to av de andre punktene i datasettet finnes &#xe5; ligge n&#xe6;rmest hverandre. I dette tilfellet vil disse to sl&#xe5;s sammen til et nytt cluster, og vi sitter n&#xe5; p&#xe5; to clustre med to punkter i hvert samt en mengde enkeltpunkter. Disse strukturene vil bli sl&#xe5;tt sammen med andre etter hvert som prosessen fortsetter med stadig st&#xf8;rre og mer sammensatte clustre. Sluttresultatet er en todeling av datasettet, samt et tre som representerer historikken i sammensl&#xe5;ingen og dermed optimale grupperinger p&#xe5; alle niv&#xe5;er. En delende hierarkisk metode arbeider motsatt vei, den starter ved &#xe5; dele universet i to, og fortsetter &#xe5; dele opp de resulterende undergruppene inntil enkeltpunktene er n&#xe5;dd.</p><p>I eksemplet under har vi studert aksjeutbytte for ulike aksjer gjennom fem &#xe5;r, 1986 til 1990, og vi er interessert i &#xe5; finne grad av likhet mellom dem for eventuelt &#xe5; se m&#xf8;nstre som kan brukes for &#xe5; gi sikrere beslutninger i fremtiden.</p><div id="page-272" class="page-normal" epub:type="pagebreak" title="272"></div><figure id="imggroup_143" class="image"><img id="img_143" src="images/image143.jpg" alt="image" /><figcaption id="caption_140">Figur 11.11 Dialogboks for hierarkisk clustering i JMP.</figcaption></figure><p>La oss se p&#xe5; treet (dendrogrammet) nedenfor. Nederst i hierarkiet (til venstre i grafen) ser vi de individuelle datapunktene &#x2013; i dette tilfelle aksjer. De to som grupperes f&#xf8;rst, er de som ligger n&#xe6;rmest hverandre iht. likhetsm&#xe5;lene &#x2013; de markerte aksjene <em>Kentucky U</em> og <em>Dominion R</em> &#x2013; vises som den tidligste grupperingen n&#xe5;r vi g&#xe5;r fra venstre mot h&#xf8;yre i treet &#x2013; det er disse to punktene som danner det f&#xf8;rste clusteret i en samlende algoritme.</p><figure id="imggroup_144" class="image"><img id="img_144" src="images/image144.jpg" alt="image" /><figcaption id="caption_141">Figur 11.12 Dendogram fra Hierarkisk clustering i JMP.</figcaption></figure><p>Idet vi beveger oss videre oppover i hierarkiet, ser vi at grupper kombineres videre, med andre grupper eller med individer i en prosess som ender i et toppniv&#xe5; som representerer hele datagrunnlaget. Den loddrette linjen, som kan flyttes, gjennom treet &#xab;kutter&#xbb; i dette tilfellet treet slik at det f&#xe5;r fem greiner &#x2013; dette er alts&#xe5; den beste femdelingen som algoritmen kan finne for disse dataene. En av disse <span id="page-273" class="page-normal" epub:type="pagebreak" title="273"></span>gruppene inneholder to aksjer, den neste fem, det neste clusteret inneholder kun en aksje, Detroit E &#x2013; som ogs&#xe5; synes &#xe5; oppf&#xf8;re seg temmelig atypisk.</p><p>JMP tillater fargekoding av data som ligger bak clusteringen, hvilket gir et godt bilde p&#xe5; likheter og ulikheter mellom de ulike datapunktene, hvilke variabler som er mest markante for et gitt cluster og hvilke variabler som synes &#xe5; ha avgj&#xf8;rende betydning for gruppeinndelingen. I v&#xe5;rt eksempel har hver aksje fem felter knyttet til seg, en for hver variabel som inngikk i clusteringen, m&#xf8;rk farge representerer h&#xf8;yere verdier for et gitt &#xe5;r. Vi ser at den f&#xf8;r nevnte, Detroit E, har en helt annen og m&#xf8;rkere farge og alts&#xe5; har h&#xf8;yere avkastning enn noen av de andre aksjene i de tre f&#xf8;rste &#xe5;rene 86&#x2013;88. Senere synes den &#xe5; ha normalisert og ligger de siste to &#xe5;rene p&#xe5; et lavere niv&#xe5; enn de fleste andre. Kikker vi p&#xe5; clusteret som defineres av Kansas Power &amp; Light og Minnesota Power &amp; Light, ser vi antydning til en felles <em>&#xf8;kende</em> trend gjennom de fem &#xe5;rene, dog sterkest for den f&#xf8;rste av de to. Legg merke til at clustering p&#xe5; denne m&#xe5;ten grupperer aksjene i henhold til deres <em>tidsutvikling</em> &#x2013; dette kan v&#xe6;re et glimrende eksempel p&#xe5; strukturer som det vil v&#xe6;re hensiktsmessig &#xe5; ta med videre inn i analytisk arbeid, for eksempel prediktive modeller.</p></section><section id="level3_75"><h2 id="h3_75">11.7 Variabel clustering</h2><p>Hittil har vi studert metoder som finner likheter (og ulikheter) mellom objekter i et datasett. Imidlertid finnes det ogs&#xe5; metoder som gj&#xf8;r tilsvarende p&#xe5; varia-belniv&#xe5; &#x2013; med andre ord de finner grupper av variabler som oppf&#xf8;rer seg likt. Det er flere fordeler knyttet til &#xe5; gj&#xf8;re dette, spesielt dersom vi sitter p&#xe5; sv&#xe6;rt mange variabler, og en mistanke om at et begrenset antall av disse kan brukes som <em>representanter</em> for de andre &#x2013; og derfor bidrar til &#xe5; gj&#xf8;re utvelgelsesprosessen enklere og ogs&#xe5; &#xe5; minske sannsynligheten for kovariansproblemer i modellene (se tidligere kapitler). Et eksempel kan v&#xe6;re et datasett som inneholder brutto-og nettoinntekt som to variabler. En variabel clustering vil p&#xe5; bakgrunn av korrelasjonen mellom disse legge dem i samme gruppe, eventuelt sammen med andre variabler. Man kan s&#xe5; velge kun &#xe5; beholde en eneste representant for gruppen, ved &#xe5; velge den variabelen som man har best forst&#xe5;else av, kontroll p&#xe5; kvaliteten av eller p&#xe5; basis av andre kriterier. I prinsippet er denne metoden sterkt beslektet med prinsipal komponent-analyse, som er beskrevet i et tidligere kapittel, men den har den fordelen at de originale variablene beholdes. Mens prinsipal komponent-analyse er designet for &#xe5; finne en optimal representasjon av <em>n</em> variabler ved &#xe5; bruke <em>k &lt; n</em> tall, vil variabel clustering med utvalg av <em>k</em>-variabler typisk ikke gi en like god forklart varians. Imidlertid vil bruk av originalvariabler framfor matematiske kombinasjoner av disse v&#xe6;re en stor fordel i bruk i modellerings-jobb, ikke minst med hensyn til lesbarheten av de endelige modellene.</p><div id="page-274" class="page-normal" epub:type="pagebreak" title="274"></div><aside id="sidebar_33" class="sidebar" epub:type="sidebar">
<p><strong>11.8 Oppsummering</strong></p>
<p>Dette kapitlet har omhandlet ulike metoder for segmentering av data, <em>clustre.</em> Et cluster er en naturlig gruppering av data, hvor gruppene er s&#xe5; sammenhengende og homogene som mulig.</p>
<p>Innenfor markedsf&#xf8;ring brukes clustering-metoder som en metodikk for <em>datautforskning,</em> for &#xe5; forst&#xe5; kundene bedre. Kapitlet har diskutert ikke-hierarkiske (K-Means clustering) og hierarkiske teknikker for clustering. I tillegg viser vi hvordan man fortolker og bruker clustrene videre. Avslutningsvis viser kapitlet hvordan man automatisk kan gruppere <em>p&#xe5; variabelniv&#xe5;</em> &#x2013; s&#xe5;kalt variabel clustering.</p>
</aside></section>
</body>
</html>